# ç”Ÿäº§ç¯å¢ƒä¼˜åŒ– TODO æ¸…å•

**æ–‡æ¡£åˆ›å»ºæ—¶é—´**ï¼š2025-10-10
**ç³»ç»Ÿ**ï¼šæ•°æ®äº¤æ˜“æ¨èç³»ç»Ÿ
**è´Ÿè´£äºº**ï¼šå¾…åˆ†é…

---

## ğŸ“‹ æ€»è§ˆ

| ä¼˜å…ˆçº§ | ç±»åˆ« | ä»»åŠ¡æ•° | é¢„è®¡å·¥æ—¶ | é¢„æœŸæ”¶ç›Š |
|--------|------|--------|----------|----------|
| P0 | æ•°æ®åº“ä¼˜åŒ– | 2 | 2-3å¤© | Pipelineé€Ÿåº¦â†‘60-80% |
| P1 | ç¼“å­˜ä¼˜åŒ– | 2 | 1-2å‘¨ | å»¶è¿Ÿâ†“20-30msï¼Œå‘½ä¸­ç‡â†‘15-25% |
| P2 | æ¨¡å‹ä¼˜åŒ– | 3 | 3-4å‘¨ | å¬å›è€—æ—¶â†“40-60%ï¼Œæ¨ç†é€Ÿåº¦â†‘2-3å€ |
| P3 | éƒ¨ç½²ä¼˜åŒ– | 3 | 4-6å‘¨ | è‡ªåŠ¨æ‰©ç¼©å®¹ï¼Œå¯ç”¨æ€§â†‘ |
| P4 | ç›‘æ§å¢å¼º | 2 | æŒç»­ | é—®é¢˜åŠæ—¶å‘ç°ï¼Œé™ä½æ•…éšœæ—¶é—´ |
| P5 | æˆæœ¬ä¼˜åŒ– | 2 | 2-3ä¸ªæœˆ | æˆæœ¬â†“60-70% |

**æ€»è®¡**ï¼š14ä¸ªä¼˜åŒ–é¡¹

---

## ğŸ”´ P0 - æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ï¼ˆç«‹å³æ‰§è¡Œï¼‰

### âœ… TODO-01: MySQL ç´¢å¼•ä¼˜åŒ–

**ä¼˜å…ˆçº§**ï¼šğŸ”´ P0 - Critical
**é¢„è®¡å·¥æ—¶**ï¼š1-2å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šæ— 

#### ä»»åŠ¡æè¿°
ä¸ºä¸šåŠ¡åº“å’Œ Matomo åº“çš„æ—¶é—´åˆ—æ·»åŠ ç´¢å¼•ï¼Œä¼˜åŒ– CDC å¢é‡æŠ½å–æ€§èƒ½ã€‚

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šåœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯ç´¢å¼•åˆ›å»º
  ```sql
  -- åœ¨ä»åº“æˆ–æµ‹è¯•åº“æ‰§è¡Œ
  EXPLAIN SELECT * FROM user WHERE update_time > '2025-10-01';
  ```

- [ ] **æ­¥éª¤2**ï¼šåˆ›å»ºä¸šåŠ¡åº“ç´¢å¼•
  ```sql
  -- è¿æ¥åˆ°ä¸šåŠ¡åº“
  USE dianshu_backend;

  -- æ·»åŠ ç´¢å¼•ï¼ˆéé«˜å³°æœŸæ‰§è¡Œï¼‰
  ALTER TABLE user ADD INDEX idx_update_time (update_time);
  ALTER TABLE dataset ADD INDEX idx_update_time (update_time);
  ALTER TABLE order_tab ADD INDEX idx_create_time (create_time);
  ALTER TABLE api_order ADD INDEX idx_create_time (create_time);
  ALTER TABLE dataset_image ADD INDEX idx_update_time (update_time);

  -- éªŒè¯ç´¢å¼•æ˜¯å¦åˆ›å»ºæˆåŠŸ
  SHOW INDEX FROM user WHERE Key_name = 'idx_update_time';
  ```

- [ ] **æ­¥éª¤3**ï¼šåˆ›å»º Matomo åº“ç´¢å¼•
  ```sql
  -- è¿æ¥åˆ° Matomo åº“
  USE matomo;

  -- æ·»åŠ ç´¢å¼•
  ALTER TABLE matomo_log_visit
    ADD INDEX idx_visit_time (visit_last_action_time);

  ALTER TABLE matomo_log_link_visit_action
    ADD INDEX idx_server_time (server_time);

  ALTER TABLE matomo_log_conversion
    ADD INDEX idx_server_time (server_time);

  -- è”åˆç´¢å¼•ï¼ˆæ”¯æŒç”¨æˆ·è¡Œä¸ºæŸ¥è¯¢ï¼‰
  ALTER TABLE matomo_log_visit
    ADD INDEX idx_user_time (idvisitor, visit_last_action_time);
  ```

- [ ] **æ­¥éª¤4**ï¼šéªŒè¯æ€§èƒ½æå‡
  ```bash
  # æ‰§è¡Œç´¢å¼•ä¼˜åŒ–å‰çš„ pipeline åŸºçº¿æµ‹è¯•
  time python -m pipeline.extract_load

  # è®°å½•æ‰§è¡Œæ—¶é—´ï¼š____ ç§’

  # åˆ›å»ºç´¢å¼•åé‡æ–°æµ‹è¯•
  time python -m pipeline.extract_load

  # è®°å½•æ‰§è¡Œæ—¶é—´ï¼š____ ç§’
  # è®¡ç®—æå‡æ¯”ä¾‹ï¼š____ %
  ```

- [ ] **æ­¥éª¤5**ï¼šç›‘æ§ç´¢å¼•ä½¿ç”¨æƒ…å†µ
  ```sql
  -- æŸ¥çœ‹ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡ï¼ˆè¿è¡Œ1-2å¤©åï¼‰
  SELECT TABLE_NAME, INDEX_NAME,
         ROWS_READ, ROWS_INSERTED, ROWS_UPDATED
  FROM performance_schema.table_io_waits_summary_by_index_usage
  WHERE INDEX_NAME LIKE 'idx_%time';
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] æ‰€æœ‰ç´¢å¼•åˆ›å»ºæˆåŠŸï¼Œæ— é”™è¯¯
- [x] CDC å¢é‡æŠ½å–æ—¶é—´ç¼©çŸ­ â‰¥50%
- [x] æ•°æ®åº“å†™å…¥æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™ï¼ˆ<10%ï¼‰
- [x] ç´¢å¼•åœ¨ EXPLAIN åˆ†æä¸­è¢«æ­£ç¡®ä½¿ç”¨

#### å›æ»šæ–¹æ¡ˆ
```sql
-- å¦‚æœç´¢å¼•å¯¼è‡´æ€§èƒ½é—®é¢˜ï¼Œå¯å¿«é€Ÿåˆ é™¤
DROP INDEX idx_update_time ON user;
DROP INDEX idx_update_time ON dataset;
-- ... å…¶ä»–ç´¢å¼•
```

#### å‚è€ƒæ–‡æ¡£
- ä½ç½®ï¼š`pipeline/extract_load.py:114-124`ï¼ˆå¢é‡æŠ½å–é€»è¾‘ï¼‰
- æ–‡æ¡£ï¼šMySQL ç´¢å¼•ä¼˜åŒ–æœ€ä½³å®è·µ

---

### âœ… TODO-02: æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–

**ä¼˜å…ˆçº§**ï¼šğŸ”´ P0 - Critical
**é¢„è®¡å·¥æ—¶**ï¼š0.5-1å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šæ— 

#### ä»»åŠ¡æè¿°
ä¼˜åŒ– SQLAlchemy è¿æ¥æ± é…ç½®ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼å’Œè¶…æ—¶ã€‚

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šä¿®æ”¹ `config/settings.py`
  ```python
  # åœ¨ DatabaseConfig ç±»ä¸­ä¿®æ”¹ sqlalchemy_url æ–¹æ³•
  def sqlalchemy_url(
      self,
      pool_size: int = 10,           # è¿æ¥æ± å¤§å°
      max_overflow: int = 20,        # æœ€å¤§æº¢å‡ºè¿æ¥
      pool_recycle: int = 3600,      # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰
      pool_pre_ping: bool = True     # è¿æ¥å‰ ping æµ‹è¯•
  ) -> str:
      base_url = (
          f"mysql+pymysql://{self.user}:{self.password}"
          f"@{self.host}:{self.port}/{self.name}"
      )
      params = (
          f"?pool_size={pool_size}"
          f"&max_overflow={max_overflow}"
          f"&pool_recycle={pool_recycle}"
          f"&pool_pre_ping={'true' if pool_pre_ping else 'false'}"
      )
      return base_url + params
  ```

- [ ] **æ­¥éª¤2**ï¼šæ›´æ–° `pipeline/extract_load.py`
  ```python
  # åœ¨ _export_table å‡½æ•°ä¸­ï¼ˆçº¦ 182 è¡Œï¼‰
  # ä¿®æ”¹ engine åˆ›å»ºé€»è¾‘
  from sqlalchemy.pool import QueuePool

  engine = create_engine(
      engine_url,
      poolclass=QueuePool,
      pool_size=10,
      max_overflow=20,
      pool_recycle=3600,
      pool_pre_ping=True,
      echo=False  # ç”Ÿäº§ç¯å¢ƒå…³é—­ SQL æ—¥å¿—
  )
  ```

- [ ] **æ­¥éª¤3**ï¼šæ·»åŠ è¿æ¥æ± ç›‘æ§
  ```python
  # åœ¨ pipeline/extract_load.py æ·»åŠ ç›‘æ§å‡½æ•°
  def log_pool_status(engine):
      pool = engine.pool
      LOGGER.info(
          "Connection pool status: size=%d, checked_in=%d, "
          "checked_out=%d, overflow=%d",
          pool.size(),
          pool.checkedin(),
          pool.checkedout(),
          pool.overflow()
      )
  ```

- [ ] **æ­¥éª¤4**ï¼šå‹åŠ›æµ‹è¯•
  ```bash
  # å¹¶å‘æ‰§è¡Œ pipelineï¼Œè§‚å¯Ÿè¿æ¥æ± çŠ¶æ€
  for i in {1..5}; do
    python -m pipeline.extract_load &
  done
  wait

  # æ£€æŸ¥æ˜¯å¦æœ‰è¿æ¥æ³„æ¼
  # MySQL: SHOW PROCESSLIST;
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] è¿æ¥æ± é…ç½®æ­£ç¡®ç”Ÿæ•ˆ
- [x] å¹¶å‘æ‰§è¡Œæ— è¿æ¥æ³„æ¼
- [x] è¿æ¥å›æ”¶æœºåˆ¶æ­£å¸¸å·¥ä½œ
- [x] pool_pre_ping é¿å… "MySQL has gone away" é”™è¯¯

#### ç¯å¢ƒå˜é‡é…ç½®
```bash
# å¯é€‰ï¼šåœ¨ .env ä¸­é…ç½®è¿æ¥æ± å‚æ•°
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_RECYCLE=3600
```

---

## ğŸŸ  P1 - ç¼“å­˜ç­–ç•¥ä¼˜åŒ–ï¼ˆ2å‘¨å†…ï¼‰

### âœ… TODO-03: Redis åˆ†å±‚ç¼“å­˜æ¶æ„

**ä¼˜å…ˆçº§**ï¼šğŸŸ  P1 - High
**é¢„è®¡å·¥æ—¶**ï¼š1å‘¨
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šæ— 

#### ä»»åŠ¡æè¿°
å®ç° Redis ä¸‰å±‚ç¼“å­˜æ¶æ„ï¼šçƒ­ç¼“å­˜ã€ç‰¹å¾åº“ã€å†·ç¼“å­˜ã€‚

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šåˆ›å»ºç¼“å­˜æŠ½è±¡å±‚ `app/tiered_cache.py`
  ```python
  """åˆ†å±‚ç¼“å­˜å®ç°"""
  import json
  from typing import Optional, Any
  import redis.asyncio as aioredis

  class TieredCache:
      def __init__(self, redis_url: str):
          self.hot = aioredis.from_url(f"{redis_url}/0")      # çƒ­ç¼“å­˜
          self.feature = aioredis.from_url(f"{redis_url}/1")  # ç‰¹å¾åº“
          self.cold = aioredis.from_url(f"{redis_url}/2")     # å†·ç¼“å­˜

      async def get_recommendation(
          self,
          key: str
      ) -> Optional[dict]:
          """è·å–æ¨èç»“æœï¼Œè‡ªåŠ¨æå‡çƒ­åº¦"""
          # 1. å…ˆæŸ¥çƒ­ç¼“å­˜
          result = await self.hot.get(key)
          if result:
              return json.loads(result)

          # 2. æŸ¥å†·ç¼“å­˜
          result = await self.cold.get(key)
          if result:
              # æå‡åˆ°çƒ­ç¼“å­˜ï¼ˆ5åˆ†é’Ÿï¼‰
              await self.hot.setex(key, 300, result)
              return json.loads(result)

          return None

      async def set_recommendation(
          self,
          key: str,
          value: dict,
          hot: bool = True
      ):
          """ä¿å­˜æ¨èç»“æœ"""
          data = json.dumps(value)
          if hot:
              # çƒ­æ•°æ®ï¼š5åˆ†é’Ÿ hot + 1å°æ—¶ cold
              await self.hot.setex(key, 300, data)
              await self.cold.setex(key, 3600, data)
          else:
              # å†·æ•°æ®ï¼šä»…ä¿å­˜åˆ° coldï¼ˆ24å°æ—¶ï¼‰
              await self.cold.setex(key, 86400, data)

      async def get_feature(
          self,
          key: str
      ) -> Optional[dict]:
          """è·å–ç‰¹å¾æ•°æ®ï¼ˆä» feature dbï¼‰"""
          result = await self.feature.get(key)
          return json.loads(result) if result else None

      async def set_feature(
          self,
          key: str,
          value: dict,
          ttl: int = 3600
      ):
          """ä¿å­˜ç‰¹å¾æ•°æ®ï¼ˆ1å°æ—¶ TTLï¼‰"""
          await self.feature.setex(key, ttl, json.dumps(value))
  ```

- [ ] **æ­¥éª¤2**ï¼šé›†æˆåˆ° `app/main.py`
  ```python
  # ä¿®æ”¹ load_models() å‡½æ•°
  from app.tiered_cache import TieredCache

  @app.on_event("startup")
  def load_models() -> None:
      # ... ç°æœ‰ä»£ç  ...

      # æ›¿æ¢åŸæœ‰çš„ cache åˆå§‹åŒ–
      redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
      app.state.tiered_cache = TieredCache(redis_url)

      LOGGER.info("Tiered cache initialized (hot/feature/cold)")
  ```

- [ ] **æ­¥éª¤3**ï¼šä¿®æ”¹æ¨èæ¥å£ä½¿ç”¨åˆ†å±‚ç¼“å­˜
  ```python
  # åœ¨ get_similar() å‡½æ•°ä¸­
  @app.get("/similar/{dataset_id}", response_model=SimilarResponse)
  async def get_similar(...):
      tiered_cache = getattr(state, "tiered_cache", None)

      if tiered_cache:
          cache_key = f"similar:{dataset_id}:{limit}"
          cached = await tiered_cache.get_recommendation(cache_key)
          if cached:
              metrics_tracker.track_cache_hit()
              return SimilarResponse(**cached)

          # ... è®¡ç®—æ¨è ...

          # ä¿å­˜ç»“æœï¼ˆåˆ¤æ–­æ˜¯å¦ä¸ºçƒ­ç‚¹ï¼‰
          is_hot = dataset_id in state.popular[:100]  # çƒ­é—¨ç‰©å“
          await tiered_cache.set_recommendation(
              cache_key,
              response.dict(),
              hot=is_hot
          )
  ```

- [ ] **æ­¥éª¤4**ï¼šé…ç½® Redis å®ä¾‹
  ```yaml
  # docker-compose.yml ä¿®æ”¹
  redis:
    image: redis:7-alpine
    command: >
      redis-server
      --appendonly yes
      --maxmemory 4gb
      --databases 16
      --save 900 1
      --save 300 10
    # ä¸ºä¸åŒ db é…ç½®ä¸åŒç­–ç•¥ï¼ˆéœ€è¦ Redis 7+ï¼‰
    # é€šè¿‡ CONFIG SET åŠ¨æ€é…ç½®
  ```

  ```bash
  # å¯åŠ¨åé…ç½®å„ db çš„æ·˜æ±°ç­–ç•¥
  redis-cli CONFIG SET maxmemory-policy-db-0 allkeys-lru   # çƒ­ç¼“å­˜
  redis-cli CONFIG SET maxmemory-policy-db-1 volatile-lru  # ç‰¹å¾åº“
  redis-cli CONFIG SET maxmemory-policy-db-2 allkeys-lfu   # å†·ç¼“å­˜
  ```

- [ ] **æ­¥éª¤5**ï¼šç›‘æ§ç¼“å­˜å‘½ä¸­ç‡
  ```python
  # app/metrics.py æ·»åŠ åˆ†å±‚ç¼“å­˜æŒ‡æ ‡
  cache_hit_by_tier = Counter(
      'cache_hit_by_tier_total',
      'Cache hits by tier',
      ['tier']  # hot, feature, cold
  )
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] ä¸‰å±‚ç¼“å­˜æ­£å¸¸å·¥ä½œ
- [x] çƒ­ç‚¹æ•°æ®è‡ªåŠ¨æå‡åˆ°çƒ­ç¼“å­˜
- [x] ç¼“å­˜å‘½ä¸­ç‡æå‡ â‰¥15%
- [x] P95 å»¶è¿Ÿé™ä½ â‰¥20ms

#### ç›‘æ§æŒ‡æ ‡
```promql
# Grafana æŸ¥è¯¢
# å„å±‚ç¼“å­˜å‘½ä¸­ç‡
sum(rate(cache_hit_by_tier_total{tier="hot"}[5m])) /
sum(rate(cache_requests_total[5m]))
```

---

### âœ… TODO-04: æœ¬åœ°å†…å­˜ç¼“å­˜

**ä¼˜å…ˆçº§**ï¼šğŸŸ  P1 - High
**é¢„è®¡å·¥æ—¶**ï¼š2-3å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šæ— 

#### ä»»åŠ¡æè¿°
å¯¹çƒ­æ¦œç­‰é«˜é¢‘è®¿é—®æ•°æ®å¢åŠ è¿›ç¨‹å†…ç¼“å­˜ï¼Œå‡å°‘ Redis è®¿é—®ã€‚

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šåˆ›å»ºæœ¬åœ°ç¼“å­˜æ¨¡å— `app/local_cache.py`
  ```python
  """æœ¬åœ°å†…å­˜ç¼“å­˜ï¼ˆè¿›ç¨‹çº§åˆ«ï¼‰"""
  from functools import lru_cache
  from datetime import datetime, timedelta
  from typing import Optional, Any
  import threading

  class LocalCache:
      """æ”¯æŒ TTL çš„æœ¬åœ°ç¼“å­˜"""
      def __init__(self, ttl_seconds: int = 60):
          self.cache = {}
          self.ttl = ttl_seconds
          self.lock = threading.Lock()

      def get(self, key: str) -> Optional[Any]:
          with self.lock:
              entry = self.cache.get(key)
              if not entry:
                  return None

              value, expire_time = entry
              if datetime.now() > expire_time:
                  del self.cache[key]
                  return None

              return value

      def set(self, key: str, value: Any, ttl: Optional[int] = None):
          expire_time = datetime.now() + timedelta(
              seconds=ttl or self.ttl
          )
          with self.lock:
              self.cache[key] = (value, expire_time)

      def clear(self):
          with self.lock:
              self.cache.clear()

      def size(self) -> int:
          return len(self.cache)

  # å…¨å±€å•ä¾‹
  _local_cache = LocalCache(ttl_seconds=60)

  def get_local_cache() -> LocalCache:
      return _local_cache
  ```

- [ ] **æ­¥éª¤2**ï¼šåœ¨ `app/main.py` ä¸­ä½¿ç”¨
  ```python
  from app.local_cache import get_local_cache

  @app.get("/hot/trending")
  async def get_trending(...):
      local_cache = get_local_cache()

      # 1. å…ˆæŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆ60ç§’ï¼‰
      cache_key = f"hot:trending:{timeframe}:{limit}"
      cached = local_cache.get(cache_key)
      if cached:
          return cached

      # 2. å†æŸ¥ Redis
      # ... ç°æœ‰é€»è¾‘ ...

      # 3. ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
      result = {"timeframe": timeframe, "items": items}
      local_cache.set(cache_key, result, ttl=60)

      return result
  ```

- [ ] **æ­¥éª¤3**ï¼šä¸ºæ¨¡å‹å…ƒæ•°æ®æ·»åŠ ç¼“å­˜
  ```python
  # ç¼“å­˜æ•°æ®é›†å…ƒæ•°æ®ï¼ˆå‡å°‘ SQLite æŸ¥è¯¢ï¼‰
  @lru_cache(maxsize=10000)
  def get_dataset_metadata(dataset_id: int) -> dict:
      """LRU ç¼“å­˜æ•°æ®é›†å…ƒæ•°æ®"""
      return state.metadata.get(dataset_id, {})

  # ç¼“å­˜ç”¨æˆ·ç”»åƒï¼ˆå‡å°‘ç‰¹å¾æŸ¥è¯¢ï¼‰
  @lru_cache(maxsize=5000)
  def get_user_profile(user_id: int) -> dict:
      """LRU ç¼“å­˜ç”¨æˆ·ç”»åƒ"""
      return state.user_profiles.get(user_id, {})
  ```

- [ ] **æ­¥éª¤4**ï¼šæ·»åŠ ç¼“å­˜é¢„çƒ­
  ```python
  @app.on_event("startup")
  def load_models() -> None:
      # ... ç°æœ‰ä»£ç  ...

      # ç¼“å­˜é¢„çƒ­ï¼šåŠ è½½çƒ­é—¨æ•°æ®
      local_cache = get_local_cache()

      # é¢„çƒ­çƒ­é—¨æ¦œå•
      for timeframe in ["1h", "24h"]:
          hot_items = get_hot_items_from_redis(timeframe, 20)
          local_cache.set(
              f"hot:trending:{timeframe}:20",
              {"timeframe": timeframe, "items": hot_items},
              ttl=300  # 5åˆ†é’Ÿ
          )

      LOGGER.info("Local cache preheated with %d entries",
                  local_cache.size())
  ```

- [ ] **æ­¥éª¤5**ï¼šæ·»åŠ ç›‘æ§æŒ‡æ ‡
  ```python
  # app/metrics.py
  local_cache_size = Gauge(
      'local_cache_size',
      'Number of entries in local cache'
  )

  local_cache_hit_rate = Gauge(
      'local_cache_hit_rate',
      'Local cache hit rate'
  )

  # å®šæœŸæ›´æ–°æŒ‡æ ‡
  @app.middleware("http")
  async def update_cache_metrics(request, call_next):
      response = await call_next(request)

      local_cache = get_local_cache()
      local_cache_size.set(local_cache.size())

      return response
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] æœ¬åœ°ç¼“å­˜æ­£å¸¸å·¥ä½œ
- [x] çƒ­æ¦œæ¥å£å»¶è¿Ÿé™ä½ â‰¥50%
- [x] Redis è®¿é—®é‡å‡å°‘ â‰¥30%
- [x] å†…å­˜ä½¿ç”¨å¢é•¿ <100MB

#### æ³¨æ„äº‹é¡¹
âš ï¸ **å¤šå®ä¾‹éƒ¨ç½²é—®é¢˜**ï¼šæœ¬åœ°ç¼“å­˜åœ¨å¤šä¸ª pod ä¹‹é—´ä¸å…±äº«ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®ä¸ä¸€è‡´
**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä»…ç¼“å­˜å‡†å®æ—¶æ•°æ®ï¼ˆçƒ­æ¦œã€ç»Ÿè®¡æ•°æ®ï¼‰
2. TTL è®¾ç½®è¾ƒçŸ­ï¼ˆâ‰¤60ç§’ï¼‰
3. é€šè¿‡ Redis pub/sub å®ç°ç¼“å­˜å¤±æ•ˆé€šçŸ¥

---

## ğŸŸ¡ P2 - æ¨¡å‹å’Œå¬å›ä¼˜åŒ–ï¼ˆ1ä¸ªæœˆï¼‰

### âœ… TODO-05: å¹¶è¡Œå¬å›å®ç°

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ P2 - Medium
**é¢„è®¡å·¥æ—¶**ï¼š1å‘¨
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šæ— 

#### ä»»åŠ¡æè¿°
å°†å¤šè·¯å¬å›ä»ä¸²è¡Œæ”¹ä¸ºå¼‚æ­¥å¹¶è¡Œæ‰§è¡Œï¼Œæå‡å¬å›é€Ÿåº¦ã€‚

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šé‡æ„å¬å›å‡½æ•°ä¸ºå¼‚æ­¥
  ```python
  # app/recall_parallel.pyï¼ˆæ–°å»ºæ–‡ä»¶ï¼‰
  import asyncio
  from typing import Dict, List, Tuple

  async def behavior_recall(
      target_id: int,
      behavior_index: Dict,
      limit: int,
      weight: float
  ) -> Tuple[Dict[int, float], Dict[int, str]]:
      """è¡Œä¸ºå¬å›ï¼ˆå¼‚æ­¥ï¼‰"""
      scores = {}
      reasons = {}

      neighbors = behavior_index.get(target_id, {})
      for item_id, score in neighbors.items():
          scores[int(item_id)] = float(score) * weight
          reasons[int(item_id)] = "behavior"

      return scores, reasons

  async def content_recall(
      target_id: int,
      content_index: Dict,
      limit: int,
      weight: float
  ) -> Tuple[Dict[int, float], Dict[int, str]]:
      """å†…å®¹å¬å›ï¼ˆå¼‚æ­¥ï¼‰"""
      # ... ç±»ä¼¼å®ç° ...
      pass

  async def vector_recall(
      target_id: int,
      vector_index: Dict,
      limit: int,
      weight: float
  ) -> Tuple[Dict[int, float], Dict[int, str]]:
      """å‘é‡å¬å›ï¼ˆå¼‚æ­¥ï¼‰"""
      # ... ç±»ä¼¼å®ç° ...
      pass

  async def parallel_recall(
      target_id: int,
      bundle: ModelBundle,
      limit: int,
      weights: Dict[str, float]
  ) -> Tuple[Dict[int, float], Dict[int, str]]:
      """å¹¶è¡Œæ‰§è¡Œå¤šè·¯å¬å›"""
      tasks = [
          behavior_recall(
              target_id, bundle.behavior, limit, weights['behavior']
          ),
          content_recall(
              target_id, bundle.content, limit, weights['content']
          ),
          vector_recall(
              target_id, bundle.vector, limit, weights['vector']
          ),
      ]

      # å¹¶è¡Œæ‰§è¡Œ
      results = await asyncio.gather(*tasks)

      # åˆå¹¶ç»“æœ
      merged_scores = {}
      merged_reasons = {}

      for scores, reasons in results:
          for item_id, score in scores.items():
              if item_id not in merged_scores:
                  merged_scores[item_id] = score
                  merged_reasons[item_id] = reasons[item_id]
              else:
                  merged_scores[item_id] += score
                  # åˆå¹¶åŸå› 
                  if reasons[item_id] not in merged_reasons[item_id]:
                      merged_reasons[item_id] += f"+{reasons[item_id]}"

      return merged_scores, merged_reasons
  ```

- [ ] **æ­¥éª¤2**ï¼šåœ¨ `app/main.py` ä¸­ä½¿ç”¨
  ```python
  from app.recall_parallel import parallel_recall

  @app.get("/similar/{dataset_id}")
  async def get_similar(...):
      # æ›¿æ¢åŸæœ‰çš„ _combine_scores_with_weights
      scores, reasons = await parallel_recall(
          target_id=dataset_id,
          bundle=bundle,
          limit=limit,
          weights=channel_weights
      )

      # ... åç»­å¤„ç† ...
  ```

- [ ] **æ­¥éª¤3**ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•
  ```python
  # tests/benchmark_recall.py
  import time
  import asyncio

  async def benchmark_recall():
      # ä¸²è¡Œå¬å›
      start = time.perf_counter()
      for _ in range(100):
          await sequential_recall(dataset_id=123)
      sequential_time = time.perf_counter() - start

      # å¹¶è¡Œå¬å›
      start = time.perf_counter()
      for _ in range(100):
          await parallel_recall(dataset_id=123)
      parallel_time = time.perf_counter() - start

      print(f"Sequential: {sequential_time:.3f}s")
      print(f"Parallel: {parallel_time:.3f}s")
      print(f"Speedup: {sequential_time/parallel_time:.2f}x")

  if __name__ == "__main__":
      asyncio.run(benchmark_recall())
  ```

- [ ] **æ­¥éª¤4**ï¼šæ·»åŠ ç›‘æ§
  ```python
  # app/metrics.py
  recall_duration_by_channel = Histogram(
      'recall_duration_seconds',
      'Duration of recall by channel',
      ['channel']  # behavior, content, vector
  )
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] å¹¶è¡Œå¬å›åŠŸèƒ½æ­£å¸¸
- [x] å¬å›ç»“æœä¸ä¸²è¡Œç‰ˆæœ¬ä¸€è‡´ï¼ˆA/B æµ‹è¯•ï¼‰
- [x] å¬å›è€—æ—¶é™ä½ â‰¥40%
- [x] CPU ä½¿ç”¨ç‡æ— æ˜¾è‘—ä¸Šå‡

---

### âœ… TODO-06: Faiss HNSW ç´¢å¼•ä¼˜åŒ–

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ P2 - Medium
**é¢„è®¡å·¥æ—¶**ï¼š3-5å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šTODO-05ï¼ˆå¯å¹¶è¡Œï¼‰

#### ä»»åŠ¡æè¿°
å°† Faiss å‘é‡ç´¢å¼•ä» IVF æ›¿æ¢ä¸º HNSWï¼Œæå‡æŸ¥è¯¢é€Ÿåº¦å’Œå¬å›è´¨é‡ã€‚

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šä¿®æ”¹ `pipeline/vector_recall_faiss.py`
  ```python
  import faiss
  import numpy as np

  def build_hnsw_index(
      embeddings: np.ndarray,
      M: int = 32,              # æ¯ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ•°
      efConstruction: int = 200, # æ„å»ºæ—¶çš„æœç´¢æ·±åº¦
      efSearch: int = 64         # æŸ¥è¯¢æ—¶çš„æœç´¢æ·±åº¦
  ):
      """æ„å»º HNSW ç´¢å¼•"""
      dimension = embeddings.shape[1]

      # åˆ›å»º HNSW ç´¢å¼•
      index = faiss.IndexHNSWFlat(dimension, M)
      index.hnsw.efConstruction = efConstruction

      # æ·»åŠ å‘é‡
      index.add(embeddings)

      # è®¾ç½®æŸ¥è¯¢å‚æ•°
      index.hnsw.efSearch = efSearch

      return index

  def search_hnsw_index(
      index: faiss.IndexHNSWFlat,
      query: np.ndarray,
      k: int = 50
  ) -> Tuple[np.ndarray, np.ndarray]:
      """åœ¨ HNSW ç´¢å¼•ä¸­æœç´¢"""
      distances, indices = index.search(query, k)
      return distances, indices
  ```

- [ ] **æ­¥éª¤2**ï¼šå¯¹æ¯” IVF å’Œ HNSW æ€§èƒ½
  ```python
  # tests/test_faiss_index.py
  def benchmark_faiss_index():
      # å‡†å¤‡æµ‹è¯•æ•°æ®
      embeddings = np.random.rand(10000, 128).astype('float32')
      queries = np.random.rand(100, 128).astype('float32')

      # IVF ç´¢å¼•
      ivf_index = build_ivf_index(embeddings)
      start = time.time()
      for q in queries:
          ivf_index.search(q.reshape(1, -1), 50)
      ivf_time = time.time() - start

      # HNSW ç´¢å¼•
      hnsw_index = build_hnsw_index(embeddings)
      start = time.time()
      for q in queries:
          hnsw_index.search(q.reshape(1, -1), 50)
      hnsw_time = time.time() - start

      print(f"IVF: {ivf_time:.3f}s, HNSW: {hnsw_time:.3f}s")
      print(f"Speedup: {ivf_time/hnsw_time:.2f}x")
  ```

- [ ] **æ­¥éª¤3**ï¼šè¯„ä¼°å¬å›è´¨é‡
  ```python
  # è®¡ç®— Recall@K
  def evaluate_recall_quality(index, ground_truth, k=50):
      recalls = []
      for query_id, true_neighbors in ground_truth.items():
          query_vec = get_embedding(query_id)
          _, predicted = index.search(query_vec, k)

          hit = len(set(predicted[0]) & set(true_neighbors[:k]))
          recall = hit / min(k, len(true_neighbors))
          recalls.append(recall)

      return np.mean(recalls)
  ```

- [ ] **æ­¥éª¤4**ï¼šæ›´æ–°æ¨¡å‹è®­ç»ƒ pipeline
  ```python
  # pipeline/train_models.py æˆ– recall_engine_v2.py
  if USE_FAISS:
      # ä½¿ç”¨ HNSW æ›¿ä»£ IVF
      index = build_hnsw_index(
          embeddings,
          M=32,                 # å¯é€šè¿‡é…ç½®è°ƒæ•´
          efConstruction=200,
          efSearch=64
      )

      # ä¿å­˜ç´¢å¼•
      faiss.write_index(index, "models/faiss_hnsw.index")

      # ä¿å­˜å…ƒæ•°æ®
      metadata = {
          "index_type": "HNSW",
          "M": 32,
          "efConstruction": 200,
          "efSearch": 64,
          "dimension": dimension,
          "num_vectors": len(embeddings)
      }
      save_json(metadata, "models/faiss_recall.meta.json")
  ```

- [ ] **æ­¥éª¤5**ï¼šåœ¨çº¿æœåŠ¡åŠ è½½ HNSW ç´¢å¼•
  ```python
  # app/main.py å¯åŠ¨æ—¶åŠ è½½
  def _load_faiss_index(base_dir: Path) -> Optional[faiss.Index]:
      index_path = base_dir / "faiss_hnsw.index"
      if not index_path.exists():
          return None

      try:
          index = faiss.read_index(str(index_path))

          # è®¾ç½®æŸ¥è¯¢å‚æ•°ï¼ˆè¿è¡Œæ—¶å¯è°ƒï¼‰
          if hasattr(index, 'hnsw'):
              index.hnsw.efSearch = int(
                  os.getenv("FAISS_EF_SEARCH", "64")
              )

          LOGGER.info("Loaded HNSW index with %d vectors",
                      index.ntotal)
          return index
      except Exception as exc:
          LOGGER.error("Failed to load Faiss index: %s", exc)
          return None
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] HNSW ç´¢å¼•æ„å»ºæˆåŠŸ
- [x] æŸ¥è¯¢é€Ÿåº¦æå‡ â‰¥2å€
- [x] Recall@50 æŒ‡æ ‡ â‰¥90%ï¼ˆä¸ IVF å¯¹æ¯”ï¼‰
- [x] å†…å­˜ä½¿ç”¨åˆç†ï¼ˆ<2GBï¼‰

#### å‚æ•°è°ƒä¼˜å»ºè®®
| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| M | 16-32 | è¿æ¥æ•°è¶Šå¤§ï¼Œç²¾åº¦è¶Šé«˜ä½†å†…å­˜å ç”¨ä¹Ÿè¶Šå¤§ |
| efConstruction | 200-400 | æ„å»ºè´¨é‡ï¼Œä»…å½±å“ç¦»çº¿è®­ç»ƒæ—¶é—´ |
| efSearch | 32-128 | æŸ¥è¯¢ç²¾åº¦ï¼Œå¯åœ¨çº¿è°ƒæ•´ |

---

### âœ… TODO-07: LightGBM æ¨¡å‹è½»é‡åŒ–

**ä¼˜å…ˆçº§**ï¼šğŸŸ¡ P2 - Medium
**é¢„è®¡å·¥æ—¶**ï¼š3-5å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šæ— 

#### ä»»åŠ¡æè¿°
ä¼˜åŒ– LightGBM æ¨¡å‹å‚æ•°ï¼Œåœ¨ä¿æŒç²¾åº¦çš„å‰æä¸‹æå‡æ¨ç†é€Ÿåº¦å’Œå‡å°æ¨¡å‹å¤§å°ã€‚

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šåˆ›å»ºæ¨¡å‹å¯¹æ¯”é…ç½® `config/model_configs.yaml`
  ```yaml
  # å½“å‰é…ç½®ï¼ˆbaselineï¼‰
  baseline:
    n_estimators: 100
    max_depth: 6
    num_leaves: 31
    learning_rate: 0.05
    min_child_samples: 20
    subsample: 1.0
    colsample_bytree: 1.0

  # è½»é‡åŒ–é…ç½®
  lightweight:
    n_estimators: 50          # å‡å°‘æ ‘æ•°é‡
    max_depth: 4              # é™ä½æ ‘æ·±åº¦
    num_leaves: 15            # å‡å°‘å¶å­èŠ‚ç‚¹
    learning_rate: 0.08       # æé«˜å­¦ä¹ ç‡è¡¥å¿æ ‘æ•°é‡
    min_child_samples: 100    # å¢åŠ æœ€å°æ ·æœ¬ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    subsample: 0.8            # æ•°æ®é‡‡æ ·
    colsample_bytree: 0.8     # ç‰¹å¾é‡‡æ ·
    reg_alpha: 0.1            # L1 æ­£åˆ™
    reg_lambda: 0.1           # L2 æ­£åˆ™

  # æç®€é…ç½®ï¼ˆæè‡´é€Ÿåº¦ï¼‰
  minimal:
    n_estimators: 30
    max_depth: 3
    num_leaves: 8
    learning_rate: 0.1
    min_child_samples: 200
    subsample: 0.7
    colsample_bytree: 0.7
    reg_alpha: 0.2
    reg_lambda: 0.2
  ```

- [ ] **æ­¥éª¤2**ï¼šä¿®æ”¹ `pipeline/train_models.py`
  ```python
  import yaml
  from pathlib import Path

  def load_model_config(config_name: str = "lightweight"):
      """åŠ è½½æ¨¡å‹é…ç½®"""
      config_path = Path("config/model_configs.yaml")
      with open(config_path) as f:
          configs = yaml.safe_load(f)
      return configs.get(config_name, configs['baseline'])

  def train_ranking_model_with_config(config_name: str):
      # åŠ è½½é…ç½®
      params = load_model_config(config_name)

      # è®­ç»ƒæ¨¡å‹
      model = LGBMClassifier(**params, random_state=42)
      model.fit(X_train, y_train)

      # è¯„ä¼°
      metrics = evaluate_model(model, X_test, y_test)
      metrics['config_name'] = config_name
      metrics['model_size_mb'] = get_model_size(model)
      metrics['inference_time_ms'] = benchmark_inference(model)

      return model, metrics
  ```

- [ ] **æ­¥éª¤3**ï¼šå¯¹æ¯”å®éªŒ
  ```python
  # è®­ç»ƒå¹¶å¯¹æ¯”ä¸‰ä¸ªé…ç½®
  configs = ['baseline', 'lightweight', 'minimal']
  results = []

  for config in configs:
      model, metrics = train_ranking_model_with_config(config)
      results.append(metrics)

      # ä¿å­˜æ¨¡å‹
      model_path = f"models/rank_model_{config}.pkl"
      save_pickle(model, model_path)

  # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
  comparison = pd.DataFrame(results)
  print(comparison[['config_name', 'auc', 'model_size_mb',
                    'inference_time_ms']])

  # ç¤ºä¾‹è¾“å‡ºï¼š
  # config_name    auc    model_size_mb  inference_time_ms
  # baseline      0.79      12.5           8.2
  # lightweight   0.77       6.3           3.5
  # minimal       0.74       3.1           1.8
  ```

- [ ] **æ­¥éª¤4**ï¼šç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä¼˜åŒ–ç‰¹å¾ï¼‰
  ```python
  def feature_importance_analysis(model, feature_names):
      """åˆ†æç‰¹å¾é‡è¦æ€§ï¼Œç§»é™¤å†—ä½™ç‰¹å¾"""
      importance = pd.DataFrame({
          'feature': feature_names,
          'importance': model.feature_importances_
      }).sort_values('importance', ascending=False)

      # ä»…ä¿ç•™ Top-K ç‰¹å¾
      top_k = 30  # ä» 70+ ç‰¹å¾é™åˆ° 30 ä¸ª
      important_features = importance.head(top_k)['feature'].tolist()

      return important_features

  # ä½¿ç”¨ç²¾ç®€ç‰¹å¾é‡æ–°è®­ç»ƒ
  important_features = feature_importance_analysis(
      baseline_model,
      feature_names
  )

  X_train_slim = X_train[important_features]
  X_test_slim = X_test[important_features]

  slim_model = LGBMClassifier(**lightweight_params)
  slim_model.fit(X_train_slim, y_train)
  ```

- [ ] **æ­¥éª¤5**ï¼šONNX å¯¼å‡ºå’ŒéªŒè¯
  ```python
  from skl2onnx import to_onnx
  import onnxruntime as rt

  def export_and_benchmark_onnx(model, X_sample):
      # å¯¼å‡º ONNX
      onnx_model = to_onnx(
          model,
          X_sample[:1],
          target_opset=12
      )

      # ä¿å­˜
      with open("models/rank_model_lightweight.onnx", "wb") as f:
          f.write(onnx_model.SerializeToString())

      # æ€§èƒ½æµ‹è¯•
      sess = rt.InferenceSession(onnx_model.SerializeToString())

      # Benchmark
      import time
      start = time.time()
      for _ in range(1000):
          sess.run(None, {'X': X_sample.values})
      onnx_time = time.time() - start

      # å¯¹æ¯” sklearn
      start = time.time()
      for _ in range(1000):
          model.predict_proba(X_sample)
      sklearn_time = time.time() - start

      print(f"sklearn: {sklearn_time:.3f}s")
      print(f"ONNX: {onnx_time:.3f}s")
      print(f"Speedup: {sklearn_time/onnx_time:.2f}x")
  ```

- [ ] **æ­¥éª¤6**ï¼šåœ¨çº¿æœåŠ¡é›†æˆ ONNX
  ```python
  # app/main.py
  import onnxruntime as rt

  def _load_onnx_model(path: Path) -> Optional[rt.InferenceSession]:
      if not path.exists():
          return None

      try:
          sess = rt.InferenceSession(
              str(path),
              providers=['CPUExecutionProvider']
          )
          LOGGER.info("Loaded ONNX model from %s", path)
          return sess
      except Exception as exc:
          LOGGER.error("Failed to load ONNX model: %s", exc)
          return None

  # å¯åŠ¨æ—¶åŠ è½½
  @app.on_event("startup")
  def load_models():
      # ä¼˜å…ˆåŠ è½½ ONNX æ¨¡å‹
      onnx_path = MODELS_DIR / "rank_model_lightweight.onnx"
      app.state.onnx_ranker = _load_onnx_model(onnx_path)

      # fallback åˆ° pkl æ¨¡å‹
      if not app.state.onnx_ranker:
          app.state.rank_model = _load_rank_model(
              MODELS_DIR / "rank_model.pkl"
          )
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] è½»é‡åŒ–æ¨¡å‹ AUC ä¸‹é™ <3%ï¼ˆå¦‚ 0.79â†’0.76+ï¼‰
- [x] æ¨¡å‹å¤§å°å‡å°‘ â‰¥50%
- [x] æ¨ç†é€Ÿåº¦æå‡ â‰¥2å€
- [x] ONNX å¯¼å‡ºæˆåŠŸä¸”åŠŸèƒ½æ­£å¸¸

#### A/B æµ‹è¯•æ–¹æ¡ˆ
```python
# ä½¿ç”¨å½±å­æ¨¡å‹è¿›è¡Œ A/B æµ‹è¯•
curl -X POST http://localhost:8000/models/reload \
  -H 'Content-Type: application/json' \
  -d '{
    "mode": "shadow",
    "source": "models/lightweight",
    "rollout": 0.1
  }'

# è§‚å¯Ÿ 1-2 å¤©åå¯¹æ¯”æŒ‡æ ‡
# - å»¶è¿Ÿæ”¹å–„
# - ä¸šåŠ¡æŒ‡æ ‡ï¼ˆCTR/CVRï¼‰æ˜¯å¦ä¸‹é™
```

---

## ğŸŸ¢ P3 - éƒ¨ç½²å’Œæ‰©ç¼©å®¹ä¼˜åŒ–ï¼ˆ1-2ä¸ªæœˆï¼‰

### âœ… TODO-08: HPA é…ç½®ä¼˜åŒ–

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ P3 - Low
**é¢„è®¡å·¥æ—¶**ï¼š2-3å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šKubernetes é›†ç¾¤

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šä¿®æ”¹ `k8s/hpa.yaml`
  ```yaml
  apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    name: recommendation-api-hpa
    namespace: recommendation
  spec:
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: recommendation-api
    minReplicas: 3              # æé«˜æœ€å°å‰¯æœ¬æ•°ï¼ˆä» 2â†’3ï¼‰
    maxReplicas: 20             # å¢åŠ æœ€å¤§å‰¯æœ¬æ•°ï¼ˆä» 10â†’20ï¼‰
    metrics:
    # CPU æ‰©ç¼©å®¹
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60  # é™ä½é˜ˆå€¼ï¼ˆä» 70â†’60ï¼‰

    # å†…å­˜æ‰©ç¼©å®¹
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70

    # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šP95 å»¶è¿Ÿ
    - type: Pods
      pods:
        metric:
          name: recommendation_latency_p95_milliseconds
        target:
          type: AverageValue
          averageValue: "80"  # P95 < 80ms

    # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šè¯·æ±‚ QPS
    - type: Pods
      pods:
        metric:
          name: recommendation_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"  # æ¯ä¸ª pod å¤„ç† 100 QPS

    behavior:
      scaleUp:
        stabilizationWindowSeconds: 60   # ç¼©çŸ­æ‰©å®¹ç¨³å®šçª—å£
        policies:
        - type: Percent
          value: 50                      # æ¯æ¬¡æ‰©å®¹ 50%
          periodSeconds: 60
        - type: Pods
          value: 2                       # æˆ–æ¯æ¬¡å¢åŠ  2 ä¸ª pod
          periodSeconds: 60
        selectPolicy: Max                # é€‰æ‹©æ›´æ¿€è¿›çš„ç­–ç•¥

      scaleDown:
        stabilizationWindowSeconds: 300  # å»¶é•¿ç¼©å®¹ç¨³å®šçª—å£ï¼ˆ5åˆ†é’Ÿï¼‰
        policies:
        - type: Percent
          value: 10                      # æ¯æ¬¡ç¼©å®¹ 10%
          periodSeconds: 60
        selectPolicy: Min                # é€‰æ‹©æ›´ä¿å®ˆçš„ç­–ç•¥
  ```

- [ ] **æ­¥éª¤2**ï¼šé…ç½® Prometheus Adapterï¼ˆæš´éœ²è‡ªå®šä¹‰æŒ‡æ ‡ï¼‰
  ```yaml
  # k8s/prometheus-adapter.yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: adapter-config
    namespace: monitoring
  data:
    config.yaml: |
      rules:
      # P95 å»¶è¿ŸæŒ‡æ ‡
      - seriesQuery: 'recommendation_latency_seconds_bucket'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        name:
          as: "recommendation_latency_p95_milliseconds"
        metricsQuery: 'histogram_quantile(0.95, sum(rate(recommendation_latency_seconds_bucket[2m])) by (le, pod)) * 1000'

      # QPS æŒ‡æ ‡
      - seriesQuery: 'recommendation_requests_total'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        name:
          as: "recommendation_requests_per_second"
        metricsQuery: 'sum(rate(recommendation_requests_total[1m])) by (pod)'
  ```

- [ ] **æ­¥éª¤3**ï¼šéƒ¨ç½²å¹¶éªŒè¯
  ```bash
  # åº”ç”¨ HPA é…ç½®
  kubectl apply -f k8s/hpa.yaml

  # æŸ¥çœ‹ HPA çŠ¶æ€
  kubectl get hpa recommendation-api-hpa -n recommendation

  # æŸ¥çœ‹è‡ªå®šä¹‰æŒ‡æ ‡æ˜¯å¦æ­£å¸¸
  kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/recommendation/pods/*/recommendation_latency_p95_milliseconds" | jq .

  # å‹åŠ›æµ‹è¯•éªŒè¯æ‰©å®¹
  hey -z 60s -c 100 http://<ingress>/api/similar/123

  # è§‚å¯Ÿ pod æ•°é‡å˜åŒ–
  kubectl get pods -n recommendation -w
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] HPA é…ç½®æ­£ç¡®éƒ¨ç½²
- [x] è‡ªå®šä¹‰æŒ‡æ ‡æ­£å¸¸é‡‡é›†
- [x] å‹åŠ›æµ‹è¯•æ—¶è‡ªåŠ¨æ‰©å®¹
- [x] è´Ÿè½½é™ä½æ—¶å¹³æ»‘ç¼©å®¹

---

### âœ… TODO-09: èµ„æºé™åˆ¶ä¼˜åŒ–

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ P3 - Low
**é¢„è®¡å·¥æ—¶**ï¼š1-2å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šTODO-08

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šä¿®æ”¹ `k8s/deployment.yaml`
  ```yaml
  containers:
  - name: recommendation-api
    image: recommendation-api:latest
    resources:
      requests:
        memory: "2Gi"      # æé«˜è¯·æ±‚ï¼ˆç¡®ä¿ç‰¹å¾åŠ è½½ï¼‰
        cpu: "1000m"       # 1 æ ¸ CPU
      limits:
        memory: "4Gi"      # é™åˆ¶ 4GBï¼ˆé˜²æ­¢ OOMï¼‰
        cpu: "2000m"       # 2 æ ¸ CPUï¼ˆå³°å€¼ï¼‰

    # å°±ç»ªæ¢é’ˆ
    readinessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 15
      periodSeconds: 5
      timeoutSeconds: 3
      successThreshold: 1
      failureThreshold: 3

    # å­˜æ´»æ¢é’ˆ
    livenessProbe:
      httpGet:
        path: /health
        port: 8000
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3

    # ä¼˜é›…å…³é—­
    lifecycle:
      preStop:
        exec:
          command: ["/bin/sh", "-c", "sleep 15"]
  ```

- [ ] **æ­¥éª¤2**ï¼šæ·»åŠ  PodDisruptionBudget
  ```yaml
  # k8s/pdb.yaml
  apiVersion: policy/v1
  kind: PodDisruptionBudget
  metadata:
    name: recommendation-api-pdb
    namespace: recommendation
  spec:
    minAvailable: 2      # è‡³å°‘ä¿æŒ 2 ä¸ª pod è¿è¡Œ
    selector:
      matchLabels:
        app: recommendation-api
  ```

- [ ] **æ­¥éª¤3**ï¼šé…ç½® QoS ç­‰çº§
  ```yaml
  # ç¡®ä¿ requests == limits ä»¥è·å¾— Guaranteed QoS
  # æˆ–è€… requests < limits è·å¾— Burstable QoS

  # Guaranteedï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
  resources:
    requests:
      memory: "3Gi"
      cpu: "1500m"
    limits:
      memory: "3Gi"      # ç›¸åŒ
      cpu: "1500m"       # ç›¸åŒ
  ```

- [ ] **æ­¥éª¤4**ï¼šç›‘æ§èµ„æºä½¿ç”¨
  ```bash
  # æŸ¥çœ‹å®é™…èµ„æºä½¿ç”¨
  kubectl top pods -n recommendation

  # æŸ¥çœ‹ OOM äº‹ä»¶
  kubectl get events -n recommendation | grep OOM

  # æŸ¥çœ‹ pod é‡å¯æ¬¡æ•°
  kubectl get pods -n recommendation -o json | \
    jq '.items[] | {name:.metadata.name, restarts:.status.containerStatuses[].restartCount}'
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] èµ„æºé…ç½®åˆç†ï¼ˆæ—  OOMï¼ŒCPU ä¸è¿‡é«˜ï¼‰
- [x] å°±ç»ªå’Œå­˜æ´»æ¢é’ˆæ­£å¸¸å·¥ä½œ
- [x] ä¼˜é›…å…³é—­æ— è¯·æ±‚ä¸¢å¤±
- [x] QoS ç­‰çº§ä¸º Guaranteed

---

### âœ… TODO-10: CDN è¾¹ç¼˜ç¼“å­˜

**ä¼˜å…ˆçº§**ï¼šğŸŸ¢ P3 - Low
**é¢„è®¡å·¥æ—¶**ï¼š3-5å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šCDN æœåŠ¡å•†è´¦å·

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šé…ç½® Nginx ç¼“å­˜å±‚
  ```nginx
  # k8s/nginx-cache.conf
  http {
      # ç¼“å­˜è·¯å¾„é…ç½®
      proxy_cache_path /var/cache/nginx/hot
                       levels=1:2
                       keys_zone=hot_cache:10m
                       max_size=1g
                       inactive=10m;

      upstream recommendation_backend {
          server recommendation-api-service:8000;
      }

      server {
          listen 80;

          # çƒ­æ¦œæ¥å£ç¼“å­˜
          location /api/hot/trending {
              proxy_pass http://recommendation_backend;

              # å¯ç”¨ç¼“å­˜
              proxy_cache hot_cache;
              proxy_cache_valid 200 5m;
              proxy_cache_use_stale error timeout updating;
              proxy_cache_lock on;

              # ç¼“å­˜ key
              proxy_cache_key "$request_uri";

              # è¿”å›ç¼“å­˜çŠ¶æ€
              add_header X-Cache-Status $upstream_cache_status;
              add_header Cache-Control "public, max-age=300";
          }

          # ç›¸ä¼¼æ¨èï¼ˆåŒ¿åç”¨æˆ·å¯ç¼“å­˜ï¼‰
          location /api/similar {
              proxy_pass http://recommendation_backend;

              # ä»…ç¼“å­˜åŒ¿åè¯·æ±‚
              proxy_cache hot_cache;
              proxy_cache_valid 200 3m;
              proxy_no_cache $http_authorization;

              add_header X-Cache-Status $upstream_cache_status;
          }

          # å…¶ä»–æ¥å£ä¸ç¼“å­˜
          location / {
              proxy_pass http://recommendation_backend;
          }
      }
  }
  ```

- [ ] **æ­¥éª¤2**ï¼šéƒ¨ç½² Nginx ç¼“å­˜å±‚
  ```yaml
  # k8s/nginx-cache-deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx-cache
    namespace: recommendation
  spec:
    replicas: 2
    template:
      spec:
        containers:
        - name: nginx
          image: nginx:alpine
          volumeMounts:
          - name: nginx-config
            mountPath: /etc/nginx/nginx.conf
            subPath: nginx.conf
          - name: cache-volume
            mountPath: /var/cache/nginx
        volumes:
        - name: nginx-config
          configMap:
            name: nginx-cache-config
        - name: cache-volume
          emptyDir:
            sizeLimit: 1Gi
  ```

- [ ] **æ­¥éª¤3**ï¼šCDN é…ç½®ï¼ˆä»¥ CloudFlare ä¸ºä¾‹ï¼‰
  ```yaml
  # Cloudflare Page Rules
  rules:
    - url: "https://api.example.com/api/hot/*"
      settings:
        cache_level: "cache_everything"
        edge_cache_ttl: 300        # 5 åˆ†é’Ÿ
        browser_cache_ttl: 180     # 3 åˆ†é’Ÿ

    - url: "https://api.example.com/api/similar/*"
      settings:
        cache_level: "cache_everything"
        edge_cache_ttl: 180
        browser_cache_ttl: 120
  ```

- [ ] **æ­¥éª¤4**ï¼šç›‘æ§ç¼“å­˜å‘½ä¸­ç‡
  ```python
  # app/metrics.py
  cdn_cache_hit = Counter(
      'cdn_cache_hit_total',
      'CDN cache hits',
      ['endpoint']
  )

  # è§£æ X-Cache-Status å¤´
  @app.middleware("http")
  async def track_cdn_cache(request, call_next):
      response = await call_next(request)

      cache_status = response.headers.get("X-Cache-Status")
      if cache_status in ["HIT", "STALE"]:
          endpoint = request.url.path
          cdn_cache_hit.labels(endpoint=endpoint).inc()

      return response
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] Nginx ç¼“å­˜å±‚æ­£å¸¸å·¥ä½œ
- [x] CDN ç¼“å­˜å‘½ä¸­ç‡ >60%
- [x] çƒ­æ¦œæ¥å£å»¶è¿Ÿ <20msï¼ˆCDN å‘½ä¸­æ—¶ï¼‰
- [x] ç¼“å­˜å¤±æ•ˆæœºåˆ¶æ­£å¸¸

---

## ğŸ”µ P4 - ç›‘æ§å’Œå‘Šè­¦å¢å¼ºï¼ˆæŒç»­ä¼˜åŒ–ï¼‰

### âœ… TODO-11: å®æ—¶æ•°æ®è´¨é‡ç›‘æ§

**ä¼˜å…ˆçº§**ï¼šğŸ”µ P4 - Nice to have
**é¢„è®¡å·¥æ—¶**ï¼š1å‘¨
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šPrometheus + AlertManager

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šå¢å¼º `pipeline/data_quality_v2.py`
  ```python
  def check_data_freshness():
      """æ£€æŸ¥æ•°æ®æ–°é²œåº¦"""
      state_path = DATA_DIR / "_metadata" / "extract_state.json"

      if not state_path.exists():
          send_alert("ERROR", "Extract state file not found")
          return

      state = json.loads(state_path.read_text())

      for source, tables in state.items():
          for table, info in tables.items():
              watermark = info.get('watermark')
              if not watermark:
                  continue

              last_update = datetime.fromisoformat(watermark)
              hours_ago = (datetime.now() - last_update).total_seconds() / 3600

              if hours_ago > 6:  # è¶…è¿‡ 6 å°æ—¶
                  send_alert(
                      "WARNING",
                      f"Table {source}.{table} not updated for {hours_ago:.1f}h"
                  )

  def check_null_ratios():
      """æ£€æŸ¥ç©ºå€¼æ¯”ä¾‹"""
      # è¯»å–æœ€æ–°çš„è´¨é‡æŠ¥å‘Š
      report_path = DATA_DIR / "evaluation" / "data_quality_report_v2.json"
      report = json.loads(report_path.read_text())

      for table, stats in report['tables'].items():
          null_ratio = stats.get('null_ratio', 0)

          if null_ratio > 0.3:  # è¶…è¿‡ 30%
              send_alert(
                  "WARNING",
                  f"Table {table} has {null_ratio:.1%} null values"
              )

  def check_data_distribution():
      """æ£€æŸ¥æ•°æ®åˆ†å¸ƒå¼‚å¸¸"""
      # æ£€æµ‹æ•°æ®å€¾æ–œ
      df = pd.read_parquet(DATA_DIR / "processed" / "interactions.parquet")

      # ç”¨æˆ·äº¤äº’åˆ†å¸ƒ
      user_counts = df.groupby('user_id').size()

      # æ£€æµ‹å¼‚å¸¸ç”¨æˆ·ï¼ˆäº¤äº’æ¬¡æ•° > å¹³å‡å€¼ + 3*æ ‡å‡†å·®ï¼‰
      mean = user_counts.mean()
      std = user_counts.std()
      threshold = mean + 3 * std

      abnormal_users = user_counts[user_counts > threshold]

      if len(abnormal_users) > 0:
          send_alert(
              "INFO",
              f"Found {len(abnormal_users)} users with abnormal behavior"
          )

  def send_alert(severity: str, message: str):
      """å‘é€å‘Šè­¦ï¼ˆé›†æˆ AlertManagerï¼‰"""
      import requests

      alert_url = os.getenv("ALERTMANAGER_URL", "http://localhost:9093")

      payload = [{
          "labels": {
              "alertname": "DataQualityIssue",
              "severity": severity.lower(),
              "service": "recommendation"
          },
          "annotations": {
              "summary": message,
              "description": f"Data quality check failed: {message}"
          }
      }]

      try:
          requests.post(f"{alert_url}/api/v1/alerts", json=payload)
      except Exception as exc:
          LOGGER.error("Failed to send alert: %s", exc)
  ```

- [ ] **æ­¥éª¤2**ï¼šæ·»åŠ å®šæ—¶ä»»åŠ¡
  ```python
  # scripts/data_quality_monitor.py
  import schedule
  import time

  def run_quality_checks():
      """è¿è¡Œæ‰€æœ‰è´¨é‡æ£€æŸ¥"""
      check_data_freshness()
      check_null_ratios()
      check_data_distribution()

  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
  schedule.every(1).hours.do(run_quality_checks)

  if __name__ == "__main__":
      while True:
          schedule.run_pending()
          time.sleep(60)
  ```

- [ ] **æ­¥éª¤3**ï¼šé…ç½® Prometheus å‘Šè­¦è§„åˆ™
  ```yaml
  # monitoring/alerts/data_quality.yml
  groups:
  - name: data_quality
    interval: 5m
    rules:
    # æ•°æ®æ–°é²œåº¦å‘Šè­¦
    - alert: DataStale
      expr: (time() - data_quality_last_run_timestamp) > 21600
      for: 5m
      labels:
        severity: warning
        team: data
      annotations:
        summary: "æ•°æ®æŠ½å–è¶…è¿‡ 6 å°æ—¶æœªæ›´æ–°"
        description: "Pipeline å¯èƒ½å¡ä½ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"

    # æ•°æ®è´¨é‡å¾—åˆ†å‘Šè­¦
    - alert: LowDataQuality
      expr: data_quality_score < 70
      for: 10m
      labels:
        severity: warning
        team: data
      annotations:
        summary: "æ•°æ®è´¨é‡å¾—åˆ†ä½äº 70: {{ $value }}"
        description: "è¯·æ£€æŸ¥ data/evaluation/data_quality_report_v2.json"

    # ç©ºå€¼ç‡å‘Šè­¦
    - alert: HighNullRatio
      expr: data_quality_null_ratio > 0.3
      for: 10m
      labels:
        severity: warning
        table: "{{ $labels.table }}"
      annotations:
        summary: "è¡¨ {{ $labels.table }} ç©ºå€¼ç‡è¶…è¿‡ 30%"

    # æ•°æ®å€¾æ–œå‘Šè­¦
    - alert: DataSkewDetected
      expr: data_quality_skew_ratio > 10
      for: 15m
      labels:
        severity: info
      annotations:
        summary: "æ£€æµ‹åˆ°æ•°æ®å€¾æ–œ"
        description: "éƒ¨åˆ†ç”¨æˆ·/ç‰©å“çš„äº¤äº’æ¬¡æ•°å¼‚å¸¸åé«˜"
  ```

- [ ] **æ­¥éª¤4**ï¼šGrafana é¢æ¿
  ```json
  {
    "dashboard": {
      "title": "Data Quality Dashboard",
      "panels": [
        {
          "title": "Data Freshness",
          "targets": [{
            "expr": "(time() - data_quality_last_run_timestamp) / 3600"
          }],
          "alert": {
            "conditions": [{"value": 6, "operator": "gt"}]
          }
        },
        {
          "title": "Null Ratio by Table",
          "targets": [{
            "expr": "data_quality_null_ratio"
          }]
        },
        {
          "title": "Data Quality Score",
          "targets": [{
            "expr": "data_quality_score"
          }]
        }
      ]
    }
  }
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] è´¨é‡æ£€æŸ¥å®šæ—¶ä»»åŠ¡è¿è¡Œæ­£å¸¸
- [x] å‘Šè­¦è§„åˆ™æ­£ç¡®è§¦å‘
- [x] Grafana é¢æ¿å±•ç¤ºæ­£å¸¸
- [x] é—®é¢˜å‘ç°æ—¶é—´ <30 åˆ†é’Ÿ

---

### âœ… TODO-12: ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

**ä¼˜å…ˆçº§**ï¼šğŸ”µ P4 - Nice to have
**é¢„è®¡å·¥æ—¶**ï¼š3-5å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šTODO-11

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šæ·»åŠ ä¸šåŠ¡æŒ‡æ ‡ `app/business_metrics.py`
  ```python
  """ä¸šåŠ¡æŒ‡æ ‡è¿½è¸ª"""
  from prometheus_client import Histogram, Counter, Gauge
  from typing import List, Set

  # æ¨èå¤šæ ·æ€§
  recommendation_diversity = Histogram(
      'recommendation_diversity_score',
      'Diversity score of recommendations',
      ['endpoint'],
      buckets=[0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
  )

  # æ–°ç‰©å“æ›å…‰
  new_item_exposure = Counter(
      'new_item_exposure_total',
      'Number of new items exposed in recommendations',
      ['endpoint']
  )

  # ç”¨æˆ·è¦†ç›–ç‡
  user_coverage = Gauge(
      'user_coverage_ratio',
      'Ratio of users receiving personalized recommendations'
  )

  # æ¨èä½ç½®åˆ†å¸ƒ
  recommendation_position = Histogram(
      'recommendation_click_position',
      'Position of clicked recommendations',
      buckets=[1, 2, 3, 5, 10, 20]
  )

  def calculate_diversity(items: List[int], metadata: dict) -> float:
      """è®¡ç®—æ¨èå¤šæ ·æ€§ï¼ˆåŸºäºç±»åˆ«/æ ‡ç­¾ï¼‰"""
      if not items:
          return 0.0

      categories = set()
      for item_id in items:
          item_meta = metadata.get(item_id, {})
          category = item_meta.get('company')
          if category:
              categories.add(category)

      # é¦™å†œç†µ or ç®€å•çš„å”¯ä¸€ç±»åˆ«æ¯”ä¾‹
      diversity = len(categories) / len(items)
      return diversity

  def track_new_item_exposure(
      items: List[int],
      new_threshold_days: int = 7
  ):
      """è¿½è¸ªæ–°ç‰©å“æ›å…‰"""
      # è¯»å–æ•°æ®é›†åˆ›å»ºæ—¶é—´
      dataset_df = pd.read_parquet("data/processed/dataset_features.parquet")

      cutoff_date = datetime.now() - timedelta(days=new_threshold_days)
      new_items = dataset_df[
          dataset_df['create_time'] > cutoff_date
      ]['dataset_id'].tolist()

      new_item_set = set(new_items)
      exposed_new_items = [i for i in items if i in new_item_set]

      return len(exposed_new_items)
  ```

- [ ] **æ­¥éª¤2**ï¼šé›†æˆåˆ°æ¨èæ¥å£
  ```python
  # app/main.py
  from app.business_metrics import (
      calculate_diversity,
      track_new_item_exposure,
      recommendation_diversity,
      new_item_exposure
  )

  @app.get("/similar/{dataset_id}")
  async def get_similar(...):
      # ... ç°æœ‰é€»è¾‘ ...

      # è®¡ç®—å¤šæ ·æ€§
      diversity = calculate_diversity(
          [item.dataset_id for item in items],
          state.metadata
      )
      recommendation_diversity.labels(endpoint="similar").observe(diversity)

      # è¿½è¸ªæ–°ç‰©å“
      new_count = track_new_item_exposure(
          [item.dataset_id for item in items]
      )
      new_item_exposure.labels(endpoint="similar").inc(new_count)

      return response
  ```

- [ ] **æ­¥éª¤3**ï¼šç”¨æˆ·è¦†ç›–ç‡ç»Ÿè®¡
  ```python
  # scripts/calculate_user_coverage.py
  import pandas as pd
  from prometheus_client import push_to_gateway

  def calculate_user_coverage():
      """è®¡ç®—ç”¨æˆ·è¦†ç›–ç‡"""
      # è¯»å–æ›å…‰æ—¥å¿—
      exposure_df = pd.read_json(
          "data/evaluation/exposure_log.jsonl",
          lines=True
      )

      # ç»Ÿè®¡æœ‰æ¨èçš„ç”¨æˆ·æ•°
      users_with_reco = exposure_df['user_id'].nunique()

      # ç»Ÿè®¡æ€»ç”¨æˆ·æ•°
      user_df = pd.read_parquet("data/business/user.parquet")
      total_users = len(user_df)

      coverage = users_with_reco / total_users if total_users > 0 else 0

      # æ¨é€åˆ° Prometheus
      from prometheus_client import CollectorRegistry, Gauge
      registry = CollectorRegistry()
      g = Gauge(
          'user_coverage_ratio',
          'User coverage ratio',
          registry=registry
      )
      g.set(coverage)

      push_to_gateway(
          'localhost:9091',
          job='user_coverage',
          registry=registry
      )

      return coverage

  # æ¯å¤©è¿è¡Œä¸€æ¬¡
  if __name__ == "__main__":
      coverage = calculate_user_coverage()
      print(f"User coverage: {coverage:.2%}")
  ```

- [ ] **æ­¥éª¤4**ï¼šGrafana ä¸šåŠ¡é¢æ¿
  ```json
  {
    "panels": [
      {
        "title": "Recommendation Diversity",
        "targets": [{
          "expr": "histogram_quantile(0.5, sum(rate(recommendation_diversity_score_bucket[5m])) by (le, endpoint))"
        }]
      },
      {
        "title": "New Item Exposure Rate",
        "targets": [{
          "expr": "sum(rate(new_item_exposure_total[5m])) / sum(rate(recommendation_count[5m]))"
        }]
      },
      {
        "title": "User Coverage",
        "targets": [{
          "expr": "user_coverage_ratio"
        }]
      }
    ]
  }
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] ä¸šåŠ¡æŒ‡æ ‡æ­£ç¡®é‡‡é›†
- [x] Grafana é¢æ¿å±•ç¤ºæ­£å¸¸
- [x] å¤šæ ·æ€§å¾—åˆ†åˆç†ï¼ˆ>0.3ï¼‰
- [x] æ–°ç‰©å“æ›å…‰ç‡ >15%

---

## âšª P5 - æˆæœ¬ä¼˜åŒ–ï¼ˆé•¿æœŸï¼‰

### âœ… TODO-13: æ•°æ®åˆ†å±‚å­˜å‚¨

**ä¼˜å…ˆçº§**ï¼šâšª P5 - Future
**é¢„è®¡å·¥æ—¶**ï¼š2-3å‘¨
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šS3/OSS å­˜å‚¨è´¦å·

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šé…ç½®å¯¹è±¡å­˜å‚¨
  ```python
  # config/storage.py
  import boto3
  from pathlib import Path

  class TieredStorage:
      def __init__(self):
          self.s3_client = boto3.client(
              's3',
              endpoint_url=os.getenv('S3_ENDPOINT'),
              aws_access_key_id=os.getenv('S3_ACCESS_KEY'),
              aws_secret_access_key=os.getenv('S3_SECRET_KEY')
          )
          self.bucket = os.getenv('S3_BUCKET', 'recommendation-data')

      def archive_old_data(
          self,
          source_table: str,
          cutoff_days: int = 30
      ):
          """å½’æ¡£æ—§æ•°æ®åˆ° S3"""
          cutoff_date = datetime.now() - timedelta(days=cutoff_days)

          # è¯»å–æ•°æ®
          df = pd.read_parquet(f"data/{source_table}.parquet")

          # åˆ†å‰²çƒ­/æ¸©æ•°æ®
          hot_df = df[df['create_time'] > cutoff_date]
          warm_df = df[df['create_time'] <= cutoff_date]

          # ä¿ç•™çƒ­æ•°æ®åœ¨ MySQL
          hot_df.to_parquet(f"data/{source_table}_hot.parquet")

          # æ¸©æ•°æ®ä¸Šä¼ åˆ° S3
          warm_path = f"warm/{source_table}/{cutoff_date.strftime('%Y%m')}.parquet"
          warm_df.to_parquet('/tmp/warm.parquet')

          self.s3_client.upload_file(
              '/tmp/warm.parquet',
              self.bucket,
              warm_path
          )

          return len(hot_df), len(warm_df)
  ```

- [ ] **æ­¥éª¤2**ï¼šä¿®æ”¹ pipeline æ”¯æŒåˆ†å±‚è¯»å–
  ```python
  # pipeline/extract_load.py
  def load_historical_data(
      table: str,
      start_date: datetime,
      end_date: datetime
  ) -> pd.DataFrame:
      """åŠ è½½å†å²æ•°æ®ï¼ˆæ”¯æŒä» S3 è¯»å–ï¼‰"""
      frames = []

      # è¯»å–çƒ­æ•°æ®ï¼ˆMySQLï¼‰
      if end_date > datetime.now() - timedelta(days=30):
          hot_df = read_from_mysql(table, start_date, end_date)
          frames.append(hot_df)

      # è¯»å–æ¸©æ•°æ®ï¼ˆS3ï¼‰
      if start_date < datetime.now() - timedelta(days=30):
          warm_df = read_from_s3(table, start_date, end_date)
          frames.append(warm_df)

      return pd.concat(frames, ignore_index=True)
  ```

- [ ] **æ­¥éª¤3**ï¼šå®šæ—¶å½’æ¡£ä»»åŠ¡
  ```python
  # scripts/archive_old_data.py
  def archive_pipeline():
      """å½’æ¡£æ—§æ•°æ®çš„å®šæ—¶ä»»åŠ¡"""
      storage = TieredStorage()

      tables = [
          'matomo_log_visit',
          'matomo_log_link_visit_action',
          'order_tab'
      ]

      for table in tables:
          hot_count, warm_count = storage.archive_old_data(
              table,
              cutoff_days=30
          )

          LOGGER.info(
              "Archived %s: hot=%d, warm=%d",
              table, hot_count, warm_count
          )

  # Cron: æ¯å‘¨è¿è¡Œä¸€æ¬¡
  # 0 2 * * 0 python scripts/archive_old_data.py
  ```

- [ ] **æ­¥éª¤4**ï¼šMySQL æ•°æ®æ¸…ç†
  ```sql
  -- åˆ é™¤å·²å½’æ¡£çš„æ—§æ•°æ®
  DELETE FROM matomo_log_visit
  WHERE server_time < DATE_SUB(NOW(), INTERVAL 30 DAY);

  -- ä¼˜åŒ–è¡¨
  OPTIMIZE TABLE matomo_log_visit;
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] æ•°æ®æˆåŠŸå½’æ¡£åˆ° S3
- [x] å†å²æ•°æ®å¯æ­£å¸¸è¯»å–
- [x] MySQL å­˜å‚¨ç©ºé—´å‡å°‘ â‰¥50%
- [x] å­˜å‚¨æˆæœ¬é™ä½ â‰¥60%

---

### âœ… TODO-14: ç‰¹å¾å­˜å‚¨å‹ç¼©

**ä¼˜å…ˆçº§**ï¼šâšª P5 - Future
**é¢„è®¡å·¥æ—¶**ï¼š2-3å¤©
**è´Ÿè´£äºº**ï¼š[ ]
**ä¾èµ–é¡¹**ï¼šæ— 

#### å®æ–½æ­¥éª¤
- [ ] **æ­¥éª¤1**ï¼šå®‰è£…å‹ç¼©åº“
  ```bash
  pip install lz4
  ```

- [ ] **æ­¥éª¤2**ï¼šåˆ›å»ºå‹ç¼©å·¥å…· `pipeline/compression.py`
  ```python
  """ç‰¹å¾å‹ç¼©å­˜å‚¨"""
  import pickle
  import lz4.frame
  from pathlib import Path
  from typing import Any

  def save_compressed(data: Any, path: Path):
      """å‹ç¼©ä¿å­˜"""
      # åºåˆ—åŒ–
      serialized = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)

      # å‹ç¼©
      compressed = lz4.frame.compress(
          serialized,
          compression_level=lz4.frame.COMPRESSIONLEVEL_MINHC
      )

      # ä¿å­˜
      path.write_bytes(compressed)

      # è¾“å‡ºå‹ç¼©æ¯”
      ratio = len(compressed) / len(serialized)
      print(f"Compression ratio: {ratio:.2%}")

  def load_compressed(path: Path) -> Any:
      """è§£å‹åŠ è½½"""
      compressed = path.read_bytes()
      serialized = lz4.frame.decompress(compressed)
      return pickle.loads(serialized)
  ```

- [ ] **æ­¥éª¤3**ï¼šä¿®æ”¹ç‰¹å¾ä¿å­˜
  ```python
  # pipeline/build_features.py
  from pipeline.compression import save_compressed, load_compressed

  # ä¿å­˜ç‰¹å¾åˆ° SQLiteï¼ˆä¿æŒåŸæ ·ï¼‰
  features_df.to_sql('user_features_v2', conn, if_exists='replace')

  # é¢å¤–ä¿å­˜å‹ç¼©ç‰ˆæœ¬ï¼ˆç”¨äºå¿«é€ŸåŠ è½½ï¼‰
  save_compressed(
      features_df,
      DATA_DIR / "processed" / "user_features_v2.lz4"
  )
  ```

- [ ] **æ­¥éª¤4**ï¼šä¿®æ”¹ API åŠ è½½é€»è¾‘
  ```python
  # app/main.py
  from pipeline.compression import load_compressed

  def _load_user_features() -> pd.DataFrame:
      # ä¼˜å…ˆåŠ è½½å‹ç¼©ç‰ˆæœ¬ï¼ˆæ›´å¿«ï¼‰
      compressed_path = DATA_DIR / "processed" / "user_features_v2.lz4"
      if compressed_path.exists():
          return load_compressed(compressed_path)

      # fallback åˆ° Parquet
      parquet_path = DATA_DIR / "processed" / "user_features_v2.parquet"
      if parquet_path.exists():
          return pd.read_parquet(parquet_path)

      # fallback åˆ° SQLite
      return _read_feature_store("SELECT * FROM user_features_v2")
  ```

- [ ] **æ­¥éª¤5**ï¼šæ‰¹é‡å‹ç¼©ç°æœ‰æ–‡ä»¶
  ```python
  # scripts/compress_features.py
  from pathlib import Path
  from pipeline.compression import save_compressed, load_compressed

  def compress_all_features():
      """å‹ç¼©æ‰€æœ‰ç‰¹å¾æ–‡ä»¶"""
      processed_dir = Path("data/processed")

      for parquet_file in processed_dir.glob("*.parquet"):
          # è¯»å–
          df = pd.read_parquet(parquet_file)

          # å‹ç¼©ä¿å­˜
          compressed_path = parquet_file.with_suffix(".lz4")
          save_compressed(df, compressed_path)

          # å¯¹æ¯”å¤§å°
          original_size = parquet_file.stat().st_size / 1024 / 1024
          compressed_size = compressed_path.stat().st_size / 1024 / 1024

          print(f"{parquet_file.name}:")
          print(f"  Original: {original_size:.1f} MB")
          print(f"  Compressed: {compressed_size:.1f} MB")
          print(f"  Ratio: {compressed_size/original_size:.1%}")

  if __name__ == "__main__":
      compress_all_features()
  ```

#### éªŒæ”¶æ ‡å‡†
- [x] å‹ç¼©ç‡ â‰¥60%ï¼ˆæ–‡ä»¶å¤§å°å‡å°‘ï¼‰
- [x] åŠ è½½é€Ÿåº¦æ— æ˜æ˜¾ä¸‹é™ï¼ˆ<10%ï¼‰
- [x] åŠŸèƒ½æ­£å¸¸ï¼Œæ— æ•°æ®ä¸¢å¤±
- [x] ç£ç›˜ç©ºé—´èŠ‚çœ â‰¥150MB

---

## ğŸ“Š è¿½è¸ªå’ŒéªŒæ”¶

### è¿›åº¦è¿½è¸ªè¡¨

| ID | ä»»åŠ¡ | ä¼˜å…ˆçº§ | çŠ¶æ€ | è´Ÿè´£äºº | å¼€å§‹æ—¥æœŸ | å®Œæˆæ—¥æœŸ | å¤‡æ³¨ |
|----|------|--------|------|--------|----------|----------|------|
| TODO-01 | MySQL ç´¢å¼•ä¼˜åŒ– | P0 | â¬œ Not Started | | | | |
| TODO-02 | è¿æ¥æ± ä¼˜åŒ– | P0 | â¬œ Not Started | | | | |
| TODO-03 | Redis åˆ†å±‚ç¼“å­˜ | P1 | â¬œ Not Started | | | | |
| TODO-04 | æœ¬åœ°å†…å­˜ç¼“å­˜ | P1 | â¬œ Not Started | | | | |
| TODO-05 | å¹¶è¡Œå¬å› | P2 | â¬œ Not Started | | | | |
| TODO-06 | Faiss HNSW | P2 | â¬œ Not Started | | | | |
| TODO-07 | LightGBM è½»é‡åŒ– | P2 | â¬œ Not Started | | | | |
| TODO-08 | HPA ä¼˜åŒ– | P3 | â¬œ Not Started | | | | |
| TODO-09 | èµ„æºé™åˆ¶ä¼˜åŒ– | P3 | â¬œ Not Started | | | | |
| TODO-10 | CDN ç¼“å­˜ | P3 | â¬œ Not Started | | | | |
| TODO-11 | æ•°æ®è´¨é‡ç›‘æ§ | P4 | â¬œ Not Started | | | | |
| TODO-12 | ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§ | P4 | â¬œ Not Started | | | | |
| TODO-13 | æ•°æ®åˆ†å±‚å­˜å‚¨ | P5 | â¬œ Not Started | | | | |
| TODO-14 | ç‰¹å¾å‹ç¼© | P5 | â¬œ Not Started | | | | |

### å…³é”®æŒ‡æ ‡åŸºçº¿å’Œç›®æ ‡

| æŒ‡æ ‡ | å½“å‰å€¼ | ç›®æ ‡å€¼ | ä¼˜åŒ–å | è¾¾æˆæ—¥æœŸ |
|------|--------|--------|--------|----------|
| API P95 å»¶è¿Ÿ | ~80ms | <50ms | | |
| ç¼“å­˜å‘½ä¸­ç‡ | ~60% | >80% | | |
| Pipeline æ‰§è¡Œæ—¶é—´ | æœªçŸ¥ | -40% | | |
| å¬å›è€—æ—¶ | æœªçŸ¥ | -50% | | |
| æ¨¡å‹æ¨ç†é€Ÿåº¦ | æœªçŸ¥ | +200% | | |
| æ•°æ®åº“å­˜å‚¨æˆæœ¬ | åŸºçº¿ | -60% | | |
| å¯ç”¨æ€§ | æœªçŸ¥ | >99.9% | | |

---

## ğŸ“ æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | æ›´æ–°å†…å®¹ | æ›´æ–°äºº |
|------|----------|--------|
| 2025-10-10 | åˆ›å»º TODO æ–‡æ¡£ | Claude Code |
| | | |
| | | |

---

## ğŸ”— å‚è€ƒèµ„æ–™

- æ¶æ„æ–‡æ¡£ï¼š`docs/ARCHITECTURE.md`
- åŸå§‹ä¼˜åŒ–å»ºè®®ï¼šåŸºäº 2025-10-10 ç³»ç»Ÿåˆ†æ
- Prometheus æœ€ä½³å®è·µï¼šhttps://prometheus.io/docs/practices/
- LightGBM è°ƒä¼˜ï¼šhttps://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html
- Faiss ç´¢å¼•é€‰æ‹©ï¼šhttps://github.com/facebookresearch/faiss/wiki/Faiss-indexes

---

**å¤‡æ³¨**ï¼š
- æœ¬æ–‡æ¡£æ˜¯åŠ¨æ€çš„ï¼Œéšç€ä¼˜åŒ–è¿›å±•æŒç»­æ›´æ–°
- æ¯ä¸ª TODO å®Œæˆåæ›´æ–°çŠ¶æ€å’Œå®Œæˆæ—¥æœŸ
- é‡è¦å†³ç­–å’Œå˜æ›´è®°å½•åœ¨å¤‡æ³¨åˆ—ä¸­
