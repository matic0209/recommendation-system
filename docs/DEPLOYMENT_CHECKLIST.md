# æ–°æœºå™¨éƒ¨ç½²æ¸…å•

æœ¬æ–‡æ¡£æä¾›åœ¨å…¨æ–°æœºå™¨ä¸Šéƒ¨ç½²æ¨èç³»ç»Ÿçš„å®Œæ•´æ­¥éª¤æ¸…å•ã€‚

---

## ğŸ“‹ éƒ¨ç½²æ¦‚è§ˆ

**ç³»ç»Ÿæ¶æ„ï¼š**
- ä¸šåŠ¡æ•°æ®åº“ï¼ˆMySQLï¼‰ï¼šå­˜å‚¨ç”¨æˆ·ã€æ•°æ®é›†ã€è®¢å•ç­‰ä¸šåŠ¡æ•°æ®
- è¡Œä¸ºæ•°æ®åº“ï¼ˆMySQLï¼Œå¯é€‰ï¼‰ï¼šMatomoè¡Œä¸ºæ—¥å¿—
- Redisï¼šç¼“å­˜å’Œç‰¹å¾å­˜å‚¨
- MLflowï¼šæ¨¡å‹ç®¡ç†å’Œå®éªŒè¿½è¸ª
- FastAPIï¼šæ¨èæœåŠ¡API
- Prometheus + Grafanaï¼šç›‘æ§å’Œå¯è§‚æµ‹æ€§
- Airflowï¼šæ•°æ®æµæ°´çº¿è°ƒåº¦

**æ ¸å¿ƒä¾èµ–è¡¨ï¼ˆdianshu_backendæ•°æ®åº“ï¼‰ï¼š**
- âœ… `task` - æ•°æ®é›†è®¢å•è¡¨
- âœ… `api_order` - APIè®¢å•è¡¨
- âœ… `dataset` - æ•°æ®é›†è¡¨
- âœ… `dataset_image` - æ•°æ®é›†å›¾ç‰‡è¡¨
- âœ… `user` - ç”¨æˆ·è¡¨
- âŒ ~~`company`~~ - å·²åˆ é™¤ï¼Œä¸å†ä½¿ç”¨
- âŒ ~~`dict`~~ - å·²åˆ é™¤ï¼Œä¸å†ä½¿ç”¨

---

## ä¸€ã€ç³»ç»Ÿè¦æ±‚

### 1.1 åŸºç¡€ç¯å¢ƒ

**æ“ä½œç³»ç»Ÿï¼š**
- Linux (æ¨è Ubuntu 20.04+ / CentOS 8+)
- macOS 11+ (ä»…ç”¨äºå¼€å‘æµ‹è¯•)

**ç¡¬ä»¶è¦æ±‚ï¼š**
- CPU: 4æ ¸+ (æ¨è8æ ¸)
- å†…å­˜: 8GB+ (æ¨è16GB)
- ç£ç›˜: 20GB+ å¯ç”¨ç©ºé—´
- ç½‘ç»œ: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

**å¿…éœ€è½¯ä»¶ï¼š**
```bash
- Python 3.8+
- Docker 20.10+
- Docker Compose v2
- Git 2.20+
- MySQL 5.7+ / 8.0+ (ä¸šåŠ¡æ•°æ®åº“)
- curl / wget (ç”¨äºæµ‹è¯•)
```

### 1.2 å®‰è£…åŸºç¡€å·¥å…·

**Ubuntu/Debian:**
```bash
# æ›´æ–°åŒ…ç®¡ç†å™¨
sudo apt update

# å®‰è£…åŸºç¡€å·¥å…·
sudo apt install -y \
    python3 \
    python3-pip \
    python3-venv \
    docker.io \
    docker-compose-v2 \
    git \
    curl \
    wget \
    mysql-client

# å¯åŠ¨Dockerå¹¶è®¾ç½®å¼€æœºè‡ªå¯
sudo systemctl start docker
sudo systemctl enable docker

# æ·»åŠ å½“å‰ç”¨æˆ·åˆ°dockerç»„ï¼ˆé¿å…æ¯æ¬¡éƒ½è¦sudoï¼‰
sudo usermod -aG docker $USER

# é‡æ–°ç™»å½•ä»¥ä½¿ç»„æƒé™ç”Ÿæ•ˆ
newgrp docker
```

**CentOS/RHEL:**
```bash
# å®‰è£…Docker
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin

# å¯åŠ¨Docker
sudo systemctl start docker
sudo systemctl enable docker

# å®‰è£…Pythonå’ŒGit
sudo yum install -y python3 python3-pip git

# æ·»åŠ ç”¨æˆ·åˆ°dockerç»„
sudo usermod -aG docker $USER
```

**macOS:**
```bash
# ä½¿ç”¨Homebrewå®‰è£…
brew install python@3.8 docker docker-compose git

# å¯åŠ¨Docker Desktop
open /Applications/Docker.app
```

---

## äºŒã€æ•°æ®æºå‡†å¤‡

### ğŸ“Œ é‡è¦è¯´æ˜

ç³»ç»Ÿæ”¯æŒä¸¤ç§æ•°æ®æºæ¨¡å¼ï¼š

1. **JSONæ–‡ä»¶æ¨¡å¼**ï¼ˆæ¨èï¼‰ï¼šä»JSONæ–‡ä»¶è¯»å–ä¸šåŠ¡æ•°æ®
2. **æ•°æ®åº“æ¨¡å¼**ï¼šä»MySQLæ•°æ®åº“å®æ—¶æŸ¥è¯¢

**é€‰æ‹©å»ºè®®ï¼š**
- å¦‚æœå·²æœ‰JSONæ•°æ®å¯¼å‡ºï¼Œæ¨èä½¿ç”¨JSONæ¨¡å¼ï¼ˆæ— éœ€æ•°æ®åº“è¿æ¥ï¼‰
- å¦‚æœéœ€è¦å®æ—¶æ•°æ®ï¼Œä½¿ç”¨æ•°æ®åº“æ¨¡å¼

### 2.1 æ•°æ®æºæ¨¡å¼ä¸€ï¼šJSONæ–‡ä»¶ï¼ˆæ¨èï¼‰

**é…ç½®æ–¹å¼ï¼š**

ç¼–è¾‘`.env`æ–‡ä»¶ï¼š
```ini
# æ•°æ®æºé…ç½®
DATA_SOURCE=json
DATA_JSON_DIR=/path/to/json/data
```

**JSONæ–‡ä»¶è¦æ±‚ï¼š**

1. **æ–‡ä»¶å‘½åè§„èŒƒï¼š**
   - å…¨é‡æ–‡ä»¶ï¼š`{table_name}.json`ï¼ˆå¦‚ `user.json`, `dataset.json`ï¼‰
   - å¢é‡æ–‡ä»¶ï¼š`{table_name}_YYYYMMDD_HHMMSS.json`ï¼ˆå¦‚ `user_20251016_140000.json`ï¼‰

2. **å¿…éœ€çš„è¡¨æ–‡ä»¶ï¼š**
   - `user.json` - ç”¨æˆ·è¡¨
   - `dataset.json` - æ•°æ®é›†è¡¨
   - `task.json` - ä»»åŠ¡è®¢å•è¡¨
   - `api_order.json` - APIè®¢å•è¡¨
   - `dataset_image.json` - æ•°æ®é›†å›¾ç‰‡è¡¨

3. **JSONæ ¼å¼ï¼š**
   ```json
   [
     {
       "id": 1,
       "user_name": "å¼ ä¸‰",
       "update_time": "2025-10-16T14:00:00"
     }
   ]
   ```

**è¯¦ç»†æ–‡æ¡£ï¼š** å‚è§ [`docs/JSON_DATA_SOURCE.md`](JSON_DATA_SOURCE.md)

### 2.2 æ•°æ®æºæ¨¡å¼äºŒï¼šMySQLæ•°æ®åº“

**ä»…åœ¨DATA_SOURCE=databaseæ—¶éœ€è¦é…ç½®**

ç¼–è¾‘`.env`æ–‡ä»¶ï¼š
```ini
# æ•°æ®æºé…ç½®
DATA_SOURCE=database

# æ•°æ®åº“è¿æ¥
BUSINESS_DB_HOST=127.0.0.1
BUSINESS_DB_PORT=3306
BUSINESS_DB_NAME=dianshu_backend
BUSINESS_DB_USER=root
BUSINESS_DB_PASSWORD=your_password
```

**æ•°æ®åº“åç§°ï¼š** `dianshu_backend`

**å¿…éœ€è¡¨ç»“æ„ï¼š**

| è¡¨å | ç”¨é€” | å…³é”®å­—æ®µ |
|-----|------|---------|
| `task` | æ•°æ®é›†è®¢å• | create_user, dataset_id, price, pay_status, pay_time |
| `api_order` | APIè®¢å• | creator_id, api_id, price, pay_status, pay_time |
| `dataset` | æ•°æ®é›†ä¿¡æ¯ | id, dataset_name, price, tag, type_id |
| `dataset_image` | æ•°æ®é›†å›¾ç‰‡ | dataset_id, image_url, image_order |
| `user` | ç”¨æˆ·ä¿¡æ¯ | id, user_name, company_name, province, city |

**åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·ï¼š**
```sql
-- åˆ›å»ºæ•°æ®åº“
CREATE DATABASE IF NOT EXISTS dianshu_backend
CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- åˆ›å»ºç”¨æˆ·å¹¶æˆæƒï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®åªè¯»æƒé™ï¼‰
CREATE USER IF NOT EXISTS 'recommend_user'@'%' IDENTIFIED BY 'your_secure_password';
GRANT SELECT ON dianshu_backend.* TO 'recommend_user'@'%';
FLUSH PRIVILEGES;
```

### 2.2 è¡Œä¸ºæ•°æ®åº“ï¼ˆå¯é€‰ï¼‰

**æ•°æ®åº“åç§°ï¼š** `matomo`

å¦‚æœæ²¡æœ‰Matomoè¡Œä¸ºæ•°æ®ï¼Œå¯ä»¥è·³è¿‡ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é™çº§ä¸ºä»…ä½¿ç”¨ä¸šåŠ¡æ•°æ®ã€‚

### 2.3 ç´¢å¼•ä¼˜åŒ–

**è¿è¡Œç´¢å¼•åˆ›å»ºè„šæœ¬ï¼ˆæå‡æŸ¥è¯¢æ€§èƒ½ï¼‰ï¼š**
```bash
# éªŒè¯ç°æœ‰ç´¢å¼•
python scripts/p0_02_verify_indexes.py

# è‡ªåŠ¨åˆ›å»ºç¼ºå¤±ç´¢å¼•ï¼ˆå®Œæ•´ç‰ˆï¼‰
python scripts/p0_02_verify_indexes.py --full

# æ‰‹åŠ¨åˆ›å»ºç´¢å¼•ï¼ˆå¯é€‰ï¼‰
mysql -h<host> -u<user> -p dianshu_backend < scripts/p0_01_add_indexes_fixed.sql
```

**å…³é”®ç´¢å¼•ï¼š**
- `task`: (create_user, dataset_id, pay_status, update_time)
- `api_order`: (creator_id, api_id, pay_status, update_time)
- `dataset`: (id, is_delete, update_time, type_id)
- `user`: (id, is_valid, update_time)

---

## ä¸‰ã€éƒ¨ç½²æ­¥éª¤

### 3.1 å…‹éš†ä»£ç ä»“åº“

```bash
# å…‹éš†åˆ°æŒ‡å®šç›®å½•
git clone <repository-url> /opt/recommend
cd /opt/recommend

# æŸ¥çœ‹å½“å‰åˆ†æ”¯
git branch

# åˆ‡æ¢åˆ°ç”Ÿäº§åˆ†æ”¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
git checkout main  # æˆ– master
```

### 3.2 é…ç½®ç¯å¢ƒå˜é‡

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim .env  # æˆ–ä½¿ç”¨ nano .env
```

**å¿…å¡«é…ç½®é¡¹ï¼š**

```ini
# ============ ä¸šåŠ¡æ•°æ®åº“é…ç½®ï¼ˆå¿…å¡«ï¼‰ ============
BUSINESS_DB_HOST=127.0.0.1          # æ•°æ®åº“ä¸»æœºåœ°å€
BUSINESS_DB_PORT=3306               # æ•°æ®åº“ç«¯å£
BUSINESS_DB_NAME=dianshu_backend    # æ•°æ®åº“åç§°
BUSINESS_DB_USER=recommend_user     # æ•°æ®åº“ç”¨æˆ·å
BUSINESS_DB_PASSWORD=your_password  # æ•°æ®åº“å¯†ç 

# ============ è¡Œä¸ºæ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼‰ ============
MATOMO_DB_HOST=127.0.0.1
MATOMO_DB_PORT=3306
MATOMO_DB_NAME=matomo
MATOMO_DB_USER=matomo_user
MATOMO_DB_PASSWORD=your_password

# ============ Redisé…ç½®ï¼ˆæ¨èï¼‰ ============
REDIS_URL=redis://127.0.0.1:6379/0
FEATURE_REDIS_URL=redis://127.0.0.1:6379/1

# ============ MLflowé…ç½®ï¼ˆå¯é€‰ï¼‰ ============
MLFLOW_TRACKING_URI=http://127.0.0.1:5000
MLFLOW_EXPERIMENT_NAME=dataset_recommendation

# ============ æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ– ============
DB_POOL_SIZE=10                     # è¿æ¥æ± å¤§å°
DB_MAX_OVERFLOW=20                  # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
DB_POOL_RECYCLE=3600                # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰
DB_POOL_PRE_PING=true               # è¿æ¥å‰æµ‹è¯•
DB_CONNECT_TIMEOUT=10               # è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰

# ============ æ—¥å¿—é…ç½® ============
LOG_LEVEL=INFO                      # DEBUG, INFO, WARNING, ERROR

# ============ ç‰¹æ€§å¼€å…³ ============
USE_FAISS_RECALL=1                  # æ˜¯å¦å¯ç”¨Faisså‘é‡å¬å›
ENABLE_METRICS=true                 # æ˜¯å¦å¯ç”¨PrometheusæŒ‡æ ‡
```

**æ•°æ®åº“è¿æ¥è¯´æ˜ï¼š**
- å¦‚æœæ•°æ®åº“åœ¨æœ¬æœºï¼šä½¿ç”¨ `127.0.0.1`
- å¦‚æœæ•°æ®åº“åœ¨Dockerå®¹å™¨ï¼šä½¿ç”¨ `host.docker.internal`ï¼ˆmacOS/Windowsï¼‰æˆ–å®¹å™¨IP
- å¦‚æœæ•°æ®åº“åœ¨è¿œç¨‹æœåŠ¡å™¨ï¼šä½¿ç”¨å®é™…IPåœ°å€

### 3.3 å®‰è£…Pythonä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python3 -m venv venv
source venv/bin/activate

# å‡çº§pip
pip install --upgrade pip

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install -r requirements.txt

# å¯é€‰ï¼šå®‰è£…å‘é‡æ£€ç´¢å’ŒONNXæ”¯æŒ
pip install faiss-cpu skl2onnx

# éªŒè¯å®‰è£…
python -c "import fastapi, pandas, sklearn, redis; print('All dependencies installed successfully!')"
```

**ä¾èµ–è¯´æ˜ï¼š**
- `fastapi + uvicorn`: APIæ¡†æ¶
- `pandas + numpy`: æ•°æ®å¤„ç†
- `scikit-learn`: æœºå™¨å­¦ä¹ 
- `pymysql + sqlalchemy`: æ•°æ®åº“è¿æ¥
- `redis + hiredis`: Rediså®¢æˆ·ç«¯
- `mlflow`: æ¨¡å‹ç®¡ç†
- `prometheus-client`: ç›‘æ§æŒ‡æ ‡
- `lightgbm`: æ’åºæ¨¡å‹
- `torch + torchvision`: å›¾åƒç‰¹å¾æå–ï¼ˆå¯é€‰ï¼‰

### 3.4 å¯åŠ¨åŸºç¡€æœåŠ¡ï¼ˆDockerï¼‰

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose logs -f redis
docker-compose logs -f mlflow
```

**å¯åŠ¨çš„æœåŠ¡ï¼š**

| æœåŠ¡ | ç«¯å£ | ç”¨é€” | å¥åº·æ£€æŸ¥ |
|-----|------|------|---------|
| Redis | 6379 | ç¼“å­˜å’Œç‰¹å¾å­˜å‚¨ | `redis-cli ping` |
| MLflow | 5000 | æ¨¡å‹ç®¡ç† | `curl http://localhost:5000` |
| Prometheus | 9090 | æŒ‡æ ‡é‡‡é›† | `curl http://localhost:9090/-/healthy` |
| Grafana | 3000 | ç›‘æ§çœ‹æ¿ | `curl http://localhost:3000/api/health` |
| Airflow Web | 8080 | æµæ°´çº¿UI | `curl http://localhost:8080/health` |
| Postgres | 5432 | Airflowå…ƒæ•°æ® | ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰ |

**éªŒè¯æœåŠ¡ï¼š**
```bash
# æ£€æŸ¥æ‰€æœ‰å®¹å™¨æ˜¯å¦è¿è¡Œ
docker-compose ps | grep Up

# æµ‹è¯•Redisè¿æ¥
redis-cli ping
# é¢„æœŸè¾“å‡º: PONG

# æµ‹è¯•MLflow
curl http://localhost:5000/api/2.0/mlflow/experiments/list
# é¢„æœŸè¾“å‡º: JSONå“åº”

# è®¿é—®ç›‘æ§é¢æ¿
curl -s http://localhost:3000/api/health | grep ok
# é¢„æœŸè¾“å‡º: {"commit":"...","database":"ok",...}
```

**å¸¸è§é—®é¢˜ï¼š**
- ç«¯å£å†²çªï¼šä¿®æ”¹ `docker-compose.yml` ä¸­çš„ç«¯å£æ˜ å°„
- æƒé™é—®é¢˜ï¼šç¡®ä¿å½“å‰ç”¨æˆ·åœ¨dockerç»„ä¸­
- å¯åŠ¨å¤±è´¥ï¼šæŸ¥çœ‹æ—¥å¿— `docker-compose logs <service-name>`

### 3.5 æ•°æ®å¤„ç†ä¸æ¨¡å‹è®­ç»ƒ

#### æ–¹å¼Aï¼šä¸€é”®è¿è¡Œå®Œæ•´Pipelineï¼ˆæ¨èï¼‰

```bash
# 1. æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰
bash scripts/run_pipeline.sh --dry-run

# 2. å…¨é‡æ‰§è¡Œï¼ˆæŠ½å–â†’æ¸…æ´—â†’ç‰¹å¾â†’è®­ç»ƒâ†’è¯„ä¼°ï¼‰
bash scripts/run_pipeline.sh

# 3. ä»…åŒæ­¥ç‰¹å¾å’Œæ¨¡å‹ï¼ˆè·³è¿‡æ•°æ®æŠ½å–ï¼‰
bash scripts/run_pipeline.sh --sync-only
```

**æ‰§è¡Œæ—¶é—´ä¼°è®¡ï¼š**
- æ•°æ®æŠ½å–ï¼š5-15åˆ†é’Ÿï¼ˆå–å†³äºæ•°æ®é‡ï¼‰
- ç‰¹å¾å·¥ç¨‹ï¼š10-30åˆ†é’Ÿ
- æ¨¡å‹è®­ç»ƒï¼š5-20åˆ†é’Ÿ
- æ€»è®¡ï¼šçº¦30-60åˆ†é’Ÿ

#### æ–¹å¼Bï¼šåˆ†æ­¥æ‰§è¡Œï¼ˆé€‚åˆè°ƒè¯•ï¼‰

```bash
# è®¾ç½®Pythonè·¯å¾„
export PYTHONPATH=/opt/recommend:$PYTHONPATH

# 1. æ•°æ®æŠ½å–ä¸åŠ è½½
python3 -m pipeline.extract_load
# äº§å‡º: data/business/*.parquet, data/matomo/*.parquet

# 2. æ•°æ®æ¸…æ´—
python3 -m pipeline.clean_data
# äº§å‡º: data/cleaned/*.parquet

# 3. å›¾åƒç‰¹å¾æå–ï¼ˆå¯é€‰ï¼Œéœ€è¦dataset_imageè¡¨æœ‰æ•°æ®ï¼‰
python3 -m pipeline.image_features
# äº§å‡º: data/processed/dataset_image_embeddings.parquet

# 4. ç‰¹å¾å·¥ç¨‹v2ï¼ˆå¢å¼ºç‰ˆï¼‰
python3 -m pipeline.build_features_v2
# äº§å‡º: data/processed/*_features_v2.parquet

# 5. æ¨¡å‹è®­ç»ƒ
python3 -m pipeline.train_models
# äº§å‡º: models/*.pkl, models/*.json, models/rank_model.onnx

# 6. å¬å›å¼•æ“æ„å»º
python3 -m pipeline.recall_engine_v2
# äº§å‡º: models/item_recall_vector.json, models/tag_to_items.json ç­‰

# 7. æ•°æ®è´¨é‡è¯„ä¼°
python3 -m pipeline.evaluate_quality_v2
# äº§å‡º: data/evaluation/data_quality_report_v2.json
```

#### é¢„æœŸäº§å‡ºæ–‡ä»¶

**æ•°æ®æ–‡ä»¶ï¼ˆ`data/` ç›®å½•ï¼‰ï¼š**
```
data/
â”œâ”€â”€ business/                           # ä¸šåŠ¡åº“æ•°æ®
â”‚   â”œâ”€â”€ task.parquet
â”‚   â”œâ”€â”€ api_order.parquet
â”‚   â”œâ”€â”€ dataset.parquet
â”‚   â””â”€â”€ user.parquet
â”œâ”€â”€ matomo/                             # è¡Œä¸ºåº“æ•°æ®ï¼ˆå¯é€‰ï¼‰
â”‚   â””â”€â”€ matomo_log_visit.parquet
â”œâ”€â”€ cleaned/                            # æ¸…æ´—åæ•°æ®
â”‚   â”œâ”€â”€ interactions_cleaned.parquet
â”‚   â”œâ”€â”€ items_cleaned.parquet
â”‚   â””â”€â”€ users_cleaned.parquet
â”œâ”€â”€ processed/                          # ç‰¹å¾æ•°æ®
â”‚   â”œâ”€â”€ interactions_features_v2.parquet
â”‚   â”œâ”€â”€ items_features_v2.parquet
â”‚   â”œâ”€â”€ users_features_v2.parquet
â”‚   â””â”€â”€ dataset_image_embeddings.parquet
â””â”€â”€ evaluation/                         # è¯„ä¼°æŠ¥å‘Š
    â”œâ”€â”€ data_quality_report_v2.json
    â”œâ”€â”€ data_quality_report.html
    â””â”€â”€ data_quality_metrics.prom
```

**æ¨¡å‹æ–‡ä»¶ï¼ˆ`models/` ç›®å½•ï¼‰ï¼š**
```
models/
â”œâ”€â”€ item_sim_behavior.pkl          # åŸºäºè¡Œä¸ºçš„ç‰©å“ç›¸ä¼¼åº¦çŸ©é˜µ
â”œâ”€â”€ item_sim_content.pkl           # åŸºäºå†…å®¹çš„ç‰©å“ç›¸ä¼¼åº¦çŸ©é˜µ
â”œâ”€â”€ user_similarity.pkl             # ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆUserCFï¼‰
â”œâ”€â”€ rank_model.pkl                  # LightGBMæ’åºæ¨¡å‹ï¼ˆPickleæ ¼å¼ï¼‰
â”œâ”€â”€ rank_model.onnx                 # LightGBMæ’åºæ¨¡å‹ï¼ˆONNXæ ¼å¼ï¼‰
â”œâ”€â”€ item_recall_vector.json        # Faisså‘é‡å¬å›ç´¢å¼•
â”œâ”€â”€ tag_to_items.json               # æ ‡ç­¾å€’æ’ç´¢å¼•
â”œâ”€â”€ item_to_tags.json               # ç‰©å“æ ‡ç­¾æ˜ å°„
â”œâ”€â”€ category_index.json             # ç±»ç›®ç´¢å¼•
â”œâ”€â”€ price_bucket_index.json        # ä»·æ ¼åŒºé—´ç´¢å¼•
â”œâ”€â”€ top_items.json                  # å…¨å±€çƒ­é—¨æ¦œå•
â”œâ”€â”€ model_registry.json             # æ¨¡å‹å…ƒä¿¡æ¯å’Œç‰ˆæœ¬
â””â”€â”€ ranking_scores_preview.json    # æ’åºåˆ†æ•°é¢„è§ˆï¼ˆç”¨äºè°ƒè¯•ï¼‰
```

**æ£€æŸ¥äº§å‡ºï¼š**
```bash
# æ£€æŸ¥æ•°æ®æ–‡ä»¶
ls -lh data/business/
ls -lh data/processed/

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -lh models/

# æŸ¥çœ‹æ¨¡å‹å…ƒä¿¡æ¯
cat models/model_registry.json | python3 -m json.tool
```

### 3.6 å¯åŠ¨æ¨èAPIæœåŠ¡

#### å¼€å‘æ¨¡å¼ï¼ˆæ”¯æŒçƒ­é‡è½½ï¼‰

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å¯åŠ¨APIï¼ˆå•è¿›ç¨‹ï¼Œæ”¯æŒä»£ç çƒ­æ›´æ–°ï¼‰
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/app.log
```

#### ç”Ÿäº§æ¨¡å¼ï¼ˆå¤šworkerï¼‰

```bash
# æ–¹å¼1ï¼šç›´æ¥è¿è¡Œ
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4

# æ–¹å¼2ï¼šä½¿ç”¨Docker
docker-compose up -d recommendation-api

# æ–¹å¼3ï¼šä½¿ç”¨Gunicornï¼ˆæ›´å¥½çš„è¿›ç¨‹ç®¡ç†ï¼‰
gunicorn app.main:app \
    --workers 4 \
    --worker-class uvicorn.workers.UvicornWorker \
    --bind 0.0.0.0:8000 \
    --timeout 120 \
    --access-logfile logs/access.log \
    --error-logfile logs/error.log
```

**Workersæ•°é‡å»ºè®®ï¼š**
- CPUå¯†é›†å‹ï¼š`workers = (CPUæ ¸å¿ƒæ•° * 2) + 1`
- IOå¯†é›†å‹ï¼š`workers = CPUæ ¸å¿ƒæ•° * 4`
- ä¸€èˆ¬å»ºè®®ï¼š4-8ä¸ªworker

**æ£€æŸ¥APIè¿›ç¨‹ï¼š**
```bash
# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep uvicorn

# æŸ¥çœ‹ç«¯å£å ç”¨
netstat -tlnp | grep 8000

# æŸ¥çœ‹èµ„æºå ç”¨
top -p $(pgrep -f uvicorn)
```

---

## å››ã€éªŒè¯éƒ¨ç½²

### 4.1 å¥åº·æ£€æŸ¥

```bash
# åŸºç¡€å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# é¢„æœŸè¾“å‡º
{
  "status": "healthy",
  "cache": "enabled",
  "models_loaded": true,
  "checks": {
    "redis": true,
    "models": true
  }
}
```

**å¥åº·æ£€æŸ¥å¤±è´¥æ’æŸ¥ï¼š**
- `models_loaded: false`: æ£€æŸ¥ `models/` ç›®å½•æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
- `redis: false`: æ£€æŸ¥Redisæ˜¯å¦è¿è¡Œ `docker-compose ps redis`
- `status: unhealthy`: æŸ¥çœ‹è¯¦ç»†æ—¥å¿— `curl http://localhost:8000/health?verbose=true`

### 4.2 APIæ¥å£æµ‹è¯•

#### 1. ä¸ªæ€§åŒ–æ¨èï¼ˆç”¨æˆ·+ç‰©å“ï¼‰

```bash
# åŸºç¡€è¯·æ±‚
curl "http://localhost:8000/recommend/detail/1?user_id=100&top_n=5"

# å®Œæ•´è¯·æ±‚ï¼ˆå¸¦æ‰€æœ‰å‚æ•°ï¼‰
curl "http://localhost:8000/recommend/detail/123?user_id=456&top_n=10&skip_viewed=true&experiment_variant=control"
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "dataset_id": 123,
  "recommendations": [
    {
      "dataset_id": 456,
      "title": "ç¤ºä¾‹æ•°æ®é›†",
      "price": 99.0,
      "cover_image": "https://example.com/image.jpg",
      "score": 0.95,
      "reason": "behavior+content+category+price+rank"
    }
  ],
  "metadata": {
    "user_id": 456,
    "algorithm": "hybrid",
    "model_version": "v1.0.0",
    "experiment_variant": "control"
  }
}
```

#### 2. ç›¸ä¼¼æ¨èï¼ˆä»…åŸºäºç‰©å“ï¼‰

```bash
# åŸºç¡€è¯·æ±‚
curl "http://localhost:8000/similar/123?top_n=10"

# å¸¦è¿‡æ»¤æ¡ä»¶
curl "http://localhost:8000/similar/123?top_n=10&min_score=0.5&max_price=100"
```

#### 3. çƒ­é—¨æ¦œå•

```bash
# 1å°æ—¶çƒ­é—¨
curl "http://localhost:8000/hot/trending?timeframe=1h&top_n=20"

# 24å°æ—¶çƒ­é—¨
curl "http://localhost:8000/hot/trending?timeframe=24h&top_n=50"

# 7å¤©çƒ­é—¨
curl "http://localhost:8000/hot/trending?timeframe=7d&top_n=100"
```

#### 4. PrometheusæŒ‡æ ‡

```bash
# è·å–æ‰€æœ‰æŒ‡æ ‡
curl http://localhost:8000/metrics

# æŸ¥çœ‹ç‰¹å®šæŒ‡æ ‡
curl http://localhost:8000/metrics | grep recommendation_requests_total
curl http://localhost:8000/metrics | grep recommendation_latency_seconds
curl http://localhost:8000/metrics | grep cache_hit_rate
```

### 4.3 æ€§èƒ½æµ‹è¯•

#### ä½¿ç”¨Apache Bench (ab)

```bash
# å®‰è£…abï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
sudo apt install apache2-utils  # Ubuntu/Debian
sudo yum install httpd-tools     # CentOS/RHEL

# æµ‹è¯•QPSï¼ˆ1000è¯·æ±‚ï¼Œ10å¹¶å‘ï¼‰
ab -n 1000 -c 10 "http://localhost:8000/similar/123?top_n=10"

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
# - Requests per second (QPS)
# - Time per request (å»¶è¿Ÿ)
# - Percentage of requests served within a certain time
```

#### ä½¿ç”¨wrk

```bash
# å®‰è£…wrk
sudo apt install wrk  # Ubuntu/Debian

# æµ‹è¯•60ç§’ï¼ˆ10è¿æ¥ï¼Œ2çº¿ç¨‹ï¼‰
wrk -t2 -c10 -d60s "http://localhost:8000/similar/123?top_n=10"

# æŸ¥çœ‹è¾“å‡º
# - Latency (50th, 75th, 90th, 99th percentile)
# - Requests/sec
# - Transfer/sec
```

#### å‹æµ‹è„šæœ¬ç¤ºä¾‹

```bash
# åˆ›å»ºå‹æµ‹è„šæœ¬
cat > load_test.sh << 'EOF'
#!/bin/bash
ENDPOINT="http://localhost:8000/similar/123?top_n=10"
DURATION=60
CONCURRENCY=20

echo "Starting load test..."
echo "Endpoint: $ENDPOINT"
echo "Duration: ${DURATION}s"
echo "Concurrency: $CONCURRENCY"

wrk -t4 -c$CONCURRENCY -d${DURATION}s $ENDPOINT

echo "Load test completed!"
EOF

# è¿è¡Œå‹æµ‹
chmod +x load_test.sh
./load_test.sh
```

**æ€§èƒ½åŸºå‡†ï¼ˆå‚è€ƒå€¼ï¼‰ï¼š**
- QPS: > 100 (å•worker)
- P95å»¶è¿Ÿ: < 100ms (æœ‰ç¼“å­˜)
- P99å»¶è¿Ÿ: < 500ms
- ç¼“å­˜å‘½ä¸­ç‡: > 80%

### 4.4 ç›‘æ§é¢æ¿æ£€æŸ¥

#### Prometheus

```bash
# è®¿é—®Prometheus
open http://localhost:9090

# æˆ–ä½¿ç”¨curlæµ‹è¯•æŸ¥è¯¢
curl -G http://localhost:9090/api/v1/query \
  --data-urlencode 'query=recommendation_requests_total'
```

**å¸¸ç”¨PromQLæŸ¥è¯¢ï¼š**
```promql
# QPS
sum(rate(recommendation_requests_total[1m]))

# æˆåŠŸç‡
sum(rate(recommendation_requests_total{status="success"}[5m]))
/
sum(rate(recommendation_requests_total[5m])) * 100

# P95å»¶è¿Ÿ
histogram_quantile(0.95,
  sum(rate(recommendation_latency_seconds_bucket[5m])) by (le, endpoint)
)

# ç¼“å­˜å‘½ä¸­ç‡
cache_hit_rate
```

#### Grafana

```bash
# è®¿é—®Grafana
open http://localhost:3000

# é»˜è®¤ç™»å½•
# ç”¨æˆ·å: admin
# å¯†ç : adminï¼ˆé¦–æ¬¡ç™»å½•éœ€ä¿®æ”¹ï¼‰
```

**å¯¼å…¥ä»ªè¡¨æ¿ï¼š**
1. ç™»å½•Grafana
2. ç‚¹å‡» "+" â†’ "Import"
3. é€‰æ‹©æ–‡ä»¶ï¼š`monitoring/grafana/dashboards/recommendation-overview.json`
4. é€‰æ‹©æ•°æ®æºï¼šPrometheus
5. ç‚¹å‡» "Import"

#### MLflow

```bash
# è®¿é—®MLflow UI
open http://localhost:5000

# æŸ¥çœ‹å®éªŒ
curl http://localhost:5000/api/2.0/mlflow/experiments/list
```

---

## äº”ã€å¸¸è§é—®é¢˜æ’æŸ¥

### 5.1 æ•°æ®åº“è¿æ¥å¤±è´¥

**ç—‡çŠ¶ï¼š**
```
pymysql.err.OperationalError: (2003, "Can't connect to MySQL server...")
```

**æ’æŸ¥æ­¥éª¤ï¼š**

1. **æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å¯è®¿é—®**
```bash
# æµ‹è¯•è¿æ¥
mysql -h<host> -P<port> -u<user> -p

# æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾
telnet <host> <port>
nc -zv <host> <port>
```

2. **æ£€æŸ¥.envé…ç½®**
```bash
cat .env | grep DB_

# éªŒè¯é…ç½®
python3 << EOF
import os
from dotenv import load_dotenv
load_dotenv()
print(f"Host: {os.getenv('BUSINESS_DB_HOST')}")
print(f"Port: {os.getenv('BUSINESS_DB_PORT')}")
print(f"Database: {os.getenv('BUSINESS_DB_NAME')}")
print(f"User: {os.getenv('BUSINESS_DB_USER')}")
EOF
```

3. **æ£€æŸ¥ç½‘ç»œå’Œé˜²ç«å¢™**
```bash
# æ£€æŸ¥é˜²ç«å¢™è§„åˆ™
sudo iptables -L -n | grep 3306
sudo firewall-cmd --list-all | grep 3306  # CentOS/RHEL

# æ£€æŸ¥MySQLç»‘å®šåœ°å€
mysql -e "SHOW VARIABLES LIKE 'bind_address';"
```

4. **æ£€æŸ¥ç”¨æˆ·æƒé™**
```sql
-- ç™»å½•MySQL
mysql -uroot -p

-- æŸ¥çœ‹ç”¨æˆ·æƒé™
SHOW GRANTS FOR 'recommend_user'@'%';

-- é‡æ–°æˆæƒï¼ˆå¦‚æœéœ€è¦ï¼‰
GRANT SELECT ON dianshu_backend.* TO 'recommend_user'@'%';
FLUSH PRIVILEGES;
```

### 5.2 æ¨¡å‹æ–‡ä»¶ç¼ºå¤±

**ç—‡çŠ¶ï¼š**
```json
{
  "status": "unhealthy",
  "models_loaded": false
}
```

**è§£å†³æ–¹æ¡ˆï¼š**

1. **æ£€æŸ¥æ¨¡å‹ç›®å½•**
```bash
ls -lh models/

# åº”åŒ…å«ä»¥ä¸‹æ–‡ä»¶
# - item_sim_behavior.pkl
# - item_sim_content.pkl
# - rank_model.pkl
# - model_registry.json
```

2. **é‡æ–°è¿è¡ŒPipeline**
```bash
# å®Œæ•´æµæ°´çº¿
bash scripts/run_pipeline.sh

# æˆ–ä»…è®­ç»ƒæ¨¡å‹
python3 -m pipeline.train_models
```

3. **ä»å¤‡ä»½æ¢å¤**
```bash
# å¦‚æœæœ‰æ¨¡å‹å¤‡ä»½
cp -r /backup/models/* ./models/

# éªŒè¯æ¨¡å‹åŠ è½½
python3 << EOF
import pickle
with open('models/item_sim_behavior.pkl', 'rb') as f:
    model = pickle.load(f)
    print(f"Model loaded successfully: {type(model)}")
EOF
```

4. **æ£€æŸ¥æ–‡ä»¶æƒé™**
```bash
# ç¡®ä¿APIè¿›ç¨‹æœ‰è¯»æƒé™
chmod -R 755 models/
chown -R $USER:$USER models/
```

### 5.3 Redisè¿æ¥å¤±è´¥

**ç—‡çŠ¶ï¼š**
```
redis.exceptions.ConnectionError: Error connecting to Redis
```

**æ’æŸ¥æ­¥éª¤ï¼š**

1. **æ£€æŸ¥RedisæœåŠ¡**
```bash
# æŸ¥çœ‹Dockerå®¹å™¨çŠ¶æ€
docker-compose ps redis

# æŸ¥çœ‹Redisæ—¥å¿—
docker-compose logs redis

# é‡å¯Redis
docker-compose restart redis
```

2. **æµ‹è¯•Redisè¿æ¥**
```bash
# ä½¿ç”¨redis-cliæµ‹è¯•
redis-cli ping
# é¢„æœŸè¾“å‡º: PONG

# æµ‹è¯•è¯»å†™
redis-cli SET test_key "test_value"
redis-cli GET test_key
redis-cli DEL test_key
```

3. **æ£€æŸ¥.envé…ç½®**
```bash
cat .env | grep REDIS

# Pythonæµ‹è¯•è¿æ¥
python3 << EOF
import redis
import os
from dotenv import load_dotenv
load_dotenv()

r = redis.from_url(os.getenv('REDIS_URL'))
r.ping()
print("Redis connection successful!")
EOF
```

4. **æ£€æŸ¥Rediså†…å­˜**
```bash
# æŸ¥çœ‹Rediså†…å­˜ä½¿ç”¨
redis-cli INFO memory

# æ¸…ç†ç¼“å­˜ï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼‰
redis-cli FLUSHDB
```

### 5.4 APIå“åº”æ…¢/è¶…æ—¶

**ç—‡çŠ¶ï¼š**
- è¯·æ±‚è¶…æ—¶ï¼ˆ> 30sï¼‰
- P99å»¶è¿Ÿè¿‡é«˜ï¼ˆ> 1sï¼‰
- ç¼“å­˜å‘½ä¸­ç‡ä½ï¼ˆ< 50%ï¼‰

**ä¼˜åŒ–æ­¥éª¤ï¼š**

1. **æ£€æŸ¥ç¼“å­˜çŠ¶æ€**
```bash
# æŸ¥çœ‹ç¼“å­˜å‘½ä¸­ç‡
curl http://localhost:8000/metrics | grep cache_hit

# é¢„çƒ­ç¼“å­˜
python3 << EOF
import requests
for dataset_id in range(1, 101):
    requests.get(f"http://localhost:8000/similar/{dataset_id}?top_n=10")
print("Cache warmed up!")
EOF
```

2. **æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½**
```bash
# è¿è¡Œç´¢å¼•ä¼˜åŒ–
python scripts/p0_02_verify_indexes.py --full

# æ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—
mysql -e "SHOW VARIABLES LIKE 'slow_query_log%';"
mysql -e "SELECT * FROM mysql.slow_log LIMIT 10;"
```

3. **è°ƒæ•´workeræ•°é‡**
```bash
# å¢åŠ workerï¼ˆå¦‚æœCPUæœ‰ä½™ï¼‰
uvicorn app.main:app --workers 8

# å‡å°‘workerï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼‰
uvicorn app.main:app --workers 2
```

4. **ç›‘æ§èµ„æºä½¿ç”¨**
```bash
# CPUå’Œå†…å­˜
top -p $(pgrep -f uvicorn)

# ç£ç›˜IO
iostat -x 1

# ç½‘ç»œ
iftop -i eth0
```

### 5.5 å†…å­˜ä¸è¶³

**ç—‡çŠ¶ï¼š**
- è¿›ç¨‹è¢«OOM Killeræ€æ­»
- `MemoryError` å¼‚å¸¸
- ç³»ç»Ÿå“åº”ç¼“æ…¢

**è§£å†³æ–¹æ¡ˆï¼š**

1. **é™åˆ¶Rediså†…å­˜**
```yaml
# åœ¨docker-compose.ymlä¸­å·²é…ç½®
redis:
  command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru
```

2. **å‡å°‘API workeræ•°é‡**
```bash
# ä»4ä¸ªå‡å°‘åˆ°2ä¸ª
uvicorn app.main:app --workers 2
```

3. **ä¼˜åŒ–æ¨¡å‹åŠ è½½**
```python
# åœ¨app/main.pyä¸­ï¼Œä½¿ç”¨å»¶è¿ŸåŠ è½½æˆ–æ¨¡å‹å‹ç¼©
# å·²å®ç°æ‡’åŠ è½½æœºåˆ¶ï¼Œæ¨¡å‹æŒ‰éœ€åŠ è½½
```

4. **ç›‘æ§å†…å­˜ä½¿ç”¨**
```bash
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
free -h

# æŸ¥çœ‹è¿›ç¨‹å†…å­˜
ps aux --sort=-%mem | head

# é‡Šæ”¾ç¼“å­˜ï¼ˆä¸´æ—¶è§£å†³ï¼‰
sudo sync && sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'
```

### 5.6 Dockerç›¸å…³é—®é¢˜

**å®¹å™¨æ— æ³•å¯åŠ¨ï¼š**
```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker-compose logs <service-name>

# é‡æ–°æ„å»ºé•œåƒ
docker-compose build --no-cache <service-name>

# æ¸…ç†æ—§å®¹å™¨å’Œç½‘ç»œ
docker-compose down -v
docker system prune -a
```

**ç«¯å£å†²çªï¼š**
```bash
# æŸ¥çœ‹ç«¯å£å ç”¨
sudo netstat -tlnp | grep <port>

# ä¿®æ”¹docker-compose.ymlä¸­çš„ç«¯å£æ˜ å°„
# ä¾‹å¦‚ï¼šå°† 8000:8000 æ”¹ä¸º 8001:8000
```

**æƒé™é—®é¢˜ï¼š**
```bash
# æ·»åŠ ç”¨æˆ·åˆ°dockerç»„
sudo usermod -aG docker $USER

# é‡æ–°ç™»å½•
newgrp docker

# æµ‹è¯•
docker ps
```

---

## å…­ã€ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å»ºè®®

### 6.1 æ•°æ®åº“ä¼˜åŒ–

1. **åˆ›å»ºåªè¯»å‰¯æœ¬**
```sql
-- ä½¿ç”¨MySQLä¸»ä»å¤åˆ¶ï¼Œæ¨èAPIè¯»å–ä»åº“
-- é…ç½®ä»åº“è¿æ¥
BUSINESS_DB_HOST=replica.example.com
```

2. **å¯ç”¨è¿æ¥æ± **
```ini
# å·²åœ¨.envä¸­é…ç½®
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40
DB_POOL_RECYCLE=3600
DB_POOL_PRE_PING=true
```

3. **ç›‘æ§æ…¢æŸ¥è¯¢**
```sql
-- å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 1;
SET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';
```

### 6.2 Redisä¼˜åŒ–

1. **æŒä¹…åŒ–é…ç½®**
```bash
# AOFæŒä¹…åŒ–ï¼ˆå·²åœ¨docker-compose.ymlé…ç½®ï¼‰
redis-server --appendonly yes --appendfsync everysec
```

2. **å†…å­˜æ·˜æ±°ç­–ç•¥**
```bash
# LRUæ·˜æ±°ï¼ˆå·²é…ç½®ï¼‰
redis-server --maxmemory 4gb --maxmemory-policy allkeys-lru
```

3. **ä¸»ä»å¤åˆ¶ï¼ˆå¯é€‰ï¼‰**
```yaml
# docker-compose.ymlæ·»åŠ Redisä»èŠ‚ç‚¹
redis-replica:
  image: redis:7-alpine
  command: redis-server --replicaof redis 6379
```

### 6.3 APIæœåŠ¡ä¼˜åŒ–

1. **ä½¿ç”¨Nginxåå‘ä»£ç†**
```nginx
upstream recommend_api {
    server 127.0.0.1:8000;
    server 127.0.0.1:8001;
    server 127.0.0.1:8002;
    server 127.0.0.1:8003;
}

server {
    listen 80;
    server_name api.example.com;

    location / {
        proxy_pass http://recommend_api;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 10s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
}
```

2. **å¯ç”¨HTTPS**
```bash
# ä½¿ç”¨Let's Encrypt
sudo apt install certbot python3-certbot-nginx
sudo certbot --nginx -d api.example.com
```

3. **é…ç½®SystemdæœåŠ¡**
```ini
# /etc/systemd/system/recommend-api.service
[Unit]
Description=Recommendation API Service
After=network.target redis.service

[Service]
Type=notify
User=recommend
Group=recommend
WorkingDirectory=/opt/recommend
Environment="PATH=/opt/recommend/venv/bin"
ExecStart=/opt/recommend/venv/bin/uvicorn app.main:app \
    --host 0.0.0.0 --port 8000 --workers 4
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

### 6.4 ç›‘æ§å’Œå‘Šè­¦

1. **é…ç½®AlertManager**
```yaml
# monitoring/alertmanager.yml
route:
  receiver: 'team-slack'
  group_by: ['alertname', 'severity']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 3h

receivers:
  - name: 'team-slack'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#alerts'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

2. **é…ç½®å‘Šè­¦è§„åˆ™**
```yaml
# monitoring/alert_rules.yml
groups:
  - name: recommendation_api
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate(recommendation_requests_total{status="error"}[5m]))
          /
          sum(rate(recommendation_requests_total[5m])) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"

      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(recommendation_latency_seconds_bucket[5m])) by (le)
          ) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "P95 latency > 1s"
```

3. **æ—¥å¿—èšåˆ**
```bash
# ä½¿ç”¨ELKæˆ–Lokiæ”¶é›†æ—¥å¿—
# é…ç½®æ—¥å¿—è½¬å‘ï¼ˆç¤ºä¾‹ï¼šä½¿ç”¨filebeatï¼‰
docker run -d \
  --name filebeat \
  -v /opt/recommend/logs:/logs:ro \
  -v ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro \
  docker.elastic.co/beats/filebeat:8.10.0
```

### 6.5 å¤‡ä»½ç­–ç•¥

1. **æ¨¡å‹å¤‡ä»½**
```bash
#!/bin/bash
# scripts/backup_models.sh
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/models"

mkdir -p $BACKUP_DIR
tar -czf $BACKUP_DIR/models_$DATE.tar.gz models/

# ä¿ç•™æœ€è¿‘30å¤©çš„å¤‡ä»½
find $BACKUP_DIR -name "models_*.tar.gz" -mtime +30 -delete

echo "Backup completed: $BACKUP_DIR/models_$DATE.tar.gz"
```

2. **æ•°æ®å¤‡ä»½**
```bash
#!/bin/bash
# scripts/backup_data.sh
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/data"

mkdir -p $BACKUP_DIR

# å¤‡ä»½Parquetæ–‡ä»¶
tar -czf $BACKUP_DIR/data_$DATE.tar.gz data/processed/

# å¤‡ä»½Redisï¼ˆRDBå¿«ç…§ï¼‰
docker exec redis redis-cli SAVE
docker cp redis:/data/dump.rdb $BACKUP_DIR/redis_$DATE.rdb

echo "Data backup completed"
```

3. **å®šæ—¶å¤‡ä»½ï¼ˆCronï¼‰**
```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ å®šæ—¶ä»»åŠ¡
# æ¯å¤©å‡Œæ™¨2ç‚¹å¤‡ä»½æ¨¡å‹
0 2 * * * /opt/recommend/scripts/backup_models.sh >> /var/log/backup.log 2>&1

# æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹å¤‡ä»½æ•°æ®
0 3 * * 0 /opt/recommend/scripts/backup_data.sh >> /var/log/backup.log 2>&1
```

### 6.6 å®‰å…¨åŠ å›º

1. **APIè®¤è¯**
```python
# åœ¨app/main.pyä¸­æ·»åŠ API Keyè®¤è¯
from fastapi import Security, HTTPException
from fastapi.security.api_key import APIKeyHeader

API_KEY_HEADER = APIKeyHeader(name="X-API-Key")

async def verify_api_key(api_key: str = Security(API_KEY_HEADER)):
    if api_key != os.getenv("API_KEY"):
        raise HTTPException(status_code=403, detail="Invalid API Key")
    return api_key
```

2. **é™æµ**
```python
# ä½¿ç”¨slowapi
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.get("/similar/{dataset_id}")
@limiter.limit("100/minute")
async def get_similar(dataset_id: int):
    ...
```

3. **é˜²ç«å¢™è§„åˆ™**
```bash
# ä»…å…è®¸ç‰¹å®šIPè®¿é—®
sudo ufw allow from 10.0.0.0/8 to any port 8000
sudo ufw enable

# æˆ–ä½¿ç”¨iptables
sudo iptables -A INPUT -p tcp --dport 8000 -s 10.0.0.0/8 -j ACCEPT
sudo iptables -A INPUT -p tcp --dport 8000 -j DROP
```

---

## ä¸ƒã€éƒ¨ç½²åæ£€æŸ¥æ¸…å•

### åŸºç¡€æ£€æŸ¥

- [ ] æ•°æ®åº“è¿æ¥æˆåŠŸ
- [ ] Redisè¿æ¥æˆåŠŸ
- [ ] æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ
- [ ] APIå¥åº·æ£€æŸ¥è¿”å› `healthy`
- [ ] æ‰€æœ‰Dockerå®¹å™¨è¿è¡Œæ­£å¸¸

### åŠŸèƒ½æ£€æŸ¥

- [ ] ä¸ªæ€§åŒ–æ¨èæ¥å£æ­£å¸¸
- [ ] ç›¸ä¼¼æ¨èæ¥å£æ­£å¸¸
- [ ] çƒ­é—¨æ¦œå•æ¥å£æ­£å¸¸
- [ ] PrometheusæŒ‡æ ‡å¯è®¿é—®
- [ ] ç¼“å­˜å‘½ä¸­ç‡ > 50%

### æ€§èƒ½æ£€æŸ¥

- [ ] QPS > 100 (å‹æµ‹)
- [ ] P95å»¶è¿Ÿ < 200ms
- [ ] P99å»¶è¿Ÿ < 500ms
- [ ] CPUä½¿ç”¨ç‡ < 70%
- [ ] å†…å­˜ä½¿ç”¨ç‡ < 80%

### ç›‘æ§æ£€æŸ¥

- [ ] Prometheusé‡‡é›†æ­£å¸¸
- [ ] Grafanaä»ªè¡¨æ¿æ˜¾ç¤ºæ­£å¸¸
- [ ] å‘Šè­¦è§„åˆ™é…ç½®å®Œæˆ
- [ ] æ—¥å¿—è¾“å‡ºæ­£å¸¸

### å®‰å…¨æ£€æŸ¥

- [ ] APIè®¤è¯å·²å¯ç”¨ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
- [ ] HTTPSå·²é…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
- [ ] é˜²ç«å¢™è§„åˆ™å·²è®¾ç½®
- [ ] æ•æ„Ÿä¿¡æ¯æœªæš´éœ²ï¼ˆ.envä¸åœ¨ä»£ç åº“ï¼‰

### å¤‡ä»½æ£€æŸ¥

- [ ] æ¨¡å‹å¤‡ä»½è„šæœ¬å·²é…ç½®
- [ ] æ•°æ®å¤‡ä»½è„šæœ¬å·²é…ç½®
- [ ] Cronå®šæ—¶ä»»åŠ¡å·²è®¾ç½®
- [ ] å¤‡ä»½æ¢å¤å·²æµ‹è¯•

---

## å…«ã€å¿«é€Ÿå‘½ä»¤å‚è€ƒ

### Dockerç®¡ç†

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# åœæ­¢æ‰€æœ‰æœåŠ¡
docker-compose down

# é‡å¯ç‰¹å®šæœåŠ¡
docker-compose restart <service-name>

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f <service-name>

# è¿›å…¥å®¹å™¨
docker-compose exec <service-name> bash

# æ¸…ç†èµ„æº
docker system prune -a
```

### Pipelineç®¡ç†

```bash
# å®Œæ•´æµæ°´çº¿
bash scripts/run_pipeline.sh

# æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
bash scripts/run_pipeline.sh --dry-run

# ä»…åŒæ­¥ç‰¹å¾å’Œæ¨¡å‹
bash scripts/run_pipeline.sh --sync-only

# å•ç‹¬æ­¥éª¤
python3 -m pipeline.extract_load
python3 -m pipeline.build_features_v2
python3 -m pipeline.train_models
```

### APIç®¡ç†

```bash
# å¯åŠ¨APIï¼ˆå¼€å‘æ¨¡å¼ï¼‰
uvicorn app.main:app --reload

# å¯åŠ¨APIï¼ˆç”Ÿäº§æ¨¡å¼ï¼‰
uvicorn app.main:app --workers 4

# çƒ­æ›´æ–°æ¨¡å‹
curl -X POST http://localhost:8000/models/reload

# æŸ¥çœ‹å¥åº·çŠ¶æ€
curl http://localhost:8000/health
```

### ç›‘æ§ç®¡ç†

```bash
# æŸ¥çœ‹æŒ‡æ ‡
curl http://localhost:8000/metrics

# æŸ¥çœ‹Prometheus
open http://localhost:9090

# æŸ¥çœ‹Grafana
open http://localhost:3000

# æŸ¥çœ‹MLflow
open http://localhost:5000
```

---

## ä¹ã€ç›¸å…³æ–‡æ¡£

- [QUICKSTART.md](../QUICKSTART.md) - å¿«é€Ÿå¼€å§‹æŒ‡å—
- [README.md](../README.md) - é¡¹ç›®æ¦‚è§ˆ
- [docs/ARCHITECTURE.md](ARCHITECTURE.md) - æ¶æ„è®¾è®¡
- [docs/API_REFERENCE.md](API_REFERENCE.md) - APIæ¥å£æ–‡æ¡£
- [docs/OPERATIONS_SOP.md](OPERATIONS_SOP.md) - è¿ç»´æ‰‹å†Œ
- [docs/PIPELINE_OVERVIEW.md](PIPELINE_OVERVIEW.md) - æµæ°´çº¿è¯¦è§£

---

## åã€æ”¯æŒä¸åé¦ˆ

- ğŸ“– æ–‡æ¡£é—®é¢˜ï¼šæäº¤ Issue åˆ° GitHub
- ğŸ› BugæŠ¥å‘Šï¼šä½¿ç”¨ GitHub Issues
- ğŸ’¡ åŠŸèƒ½å»ºè®®ï¼šé€šè¿‡ Pull Request æäº¤
- ğŸ“§ è”ç³»æ–¹å¼ï¼š[your-email@example.com]

---

**æœ€åæ›´æ–°æ—¶é—´ï¼š** 2025-10-16
**ç‰ˆæœ¬ï¼š** v1.0.0
**ç»´æŠ¤è€…ï¼š** æ¨èç³»ç»Ÿå›¢é˜Ÿ
