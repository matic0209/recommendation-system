#!/usr/bin/env python3
"""
åˆ†æ SBERT embeddings çš„ PCA é™ç»´æ•ˆæœ
å¸®åŠ©é€‰æ‹©æœ€ä¼˜çš„ PCA ç»´åº¦æ•°é‡
"""
import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer

# è®¾ç½® HuggingFace é•œåƒ
if os.getenv("HF_ENDPOINT"):
    os.environ["HF_ENDPOINT"] = os.getenv("HF_ENDPOINT")

def analyze_pca_components(max_components: int = 50):
    """
    åˆ†æä¸åŒ PCA ç»´åº¦ä¸‹çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹

    Args:
        max_components: æœ€å¤§åˆ†æçš„ä¸»æˆåˆ†æ•°é‡ï¼ˆé»˜è®¤50ï¼‰
    """
    print("=" * 70)
    print("SBERT Embeddings PCA é™ç»´åˆ†æ")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“Š Step 1: åŠ è½½æ•°æ®é›†ç‰¹å¾...")
    dataset_features_path = "data/processed/dataset_features.parquet"
    if not os.path.exists(dataset_features_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {dataset_features_path}")
        print("   è¯·å…ˆè¿è¡Œè®­ç»ƒæµç¨‹ç”Ÿæˆ dataset_features.parquet")
        sys.exit(1)

    df = pd.read_parquet(dataset_features_path)
    print(f"   âœ“ åŠ è½½äº† {len(df)} ä¸ªæ•°æ®é›†")

    # 2. ç”Ÿæˆæ–‡æœ¬ embeddings
    print("\nğŸ¤– Step 2: ç”Ÿæˆ SBERT embeddings...")
    model_name = os.getenv("SBERT_MODEL", "paraphrase-multilingual-MiniLM-L12-v2")
    print(f"   æ¨¡å‹: {model_name}")

    try:
        if not model_name.startswith("sentence-transformers/"):
            full_model_name = f"sentence-transformers/{model_name}"
        else:
            full_model_name = model_name
        model = SentenceTransformer(full_model_name, device='cpu')
    except Exception as e:
        print(f"   å°è¯•ä¸å¸¦å‰ç¼€ä¸‹è½½: {e}")
        model = SentenceTransformer(model_name, device='cpu')

    texts = (df["description"].fillna("") + " " + df["tag"].fillna("")).values
    embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)
    print(f"   âœ“ Embeddings shape: {embeddings.shape}")

    # 3. PCA åˆ†æ
    print(f"\nğŸ” Step 3: PCA é™ç»´åˆ†æ (max_components={max_components})...")
    pca = PCA(n_components=min(max_components, embeddings.shape[1]), random_state=42)
    pca.fit(embeddings)

    # 4. è®¡ç®—ç´¯ç§¯è§£é‡Šæ–¹å·®
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

    # 5. è¾“å‡ºå…³é”®ç»´åº¦çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹
    print("\n" + "=" * 70)
    print("ğŸ“ˆ æ–¹å·®è§£é‡Šæ¯”ä¾‹åˆ†æ")
    print("=" * 70)
    print(f"{'ç»´åº¦':<8} {'å•ä¸ªæ–¹å·®%':<12} {'ç´¯ç§¯æ–¹å·®%':<12} {'è¯´æ˜'}")
    print("-" * 70)

    key_dims = [1, 2, 3, 5, 10, 15, 20, 30, 40, 50]
    for n in key_dims:
        if n <= len(cumulative_variance_ratio):
            variance = explained_variance_ratio[n-1] * 100
            cumulative = cumulative_variance_ratio[n-1] * 100

            # æ·»åŠ è¯´æ˜
            if cumulative >= 95:
                note = "âœ“ ä¼˜ç§€ï¼ˆ>=95%ï¼‰"
            elif cumulative >= 90:
                note = "âœ“ è‰¯å¥½ï¼ˆ>=90%ï¼‰"
            elif cumulative >= 80:
                note = "â—‹ å¯æ¥å—ï¼ˆ>=80%ï¼‰"
            else:
                note = "â–³ ä¿¡æ¯æŸå¤±è¾ƒå¤§"

            print(f"{n:<8} {variance:>10.2f}% {cumulative:>10.2f}% {note}")

    # 6. æ¨èç»´åº¦
    print("\n" + "=" * 70)
    print("ğŸ’¡ æ¨èé…ç½®")
    print("=" * 70)

    # æ‰¾åˆ°è¾¾åˆ°ä¸åŒé˜ˆå€¼çš„æœ€å°ç»´åº¦
    thresholds = [0.80, 0.85, 0.90, 0.95]
    for threshold in thresholds:
        n_components = np.argmax(cumulative_variance_ratio >= threshold) + 1
        if cumulative_variance_ratio[n_components-1] >= threshold:
            print(f"ä¿ç•™ {threshold*100:.0f}% æ–¹å·®: TEXT_PCA_COMPONENTS={n_components}")

    # 7. ç»˜åˆ¶æ–¹å·®æ›²çº¿
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # å·¦å›¾ï¼šå•ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹
    components = range(1, len(explained_variance_ratio) + 1)
    ax1.bar(components, explained_variance_ratio * 100, alpha=0.7, color='steelblue')
    ax1.set_xlabel('ä¸»æˆåˆ†ç¼–å·', fontsize=12)
    ax1.set_ylabel('æ–¹å·®è§£é‡Šæ¯”ä¾‹ (%)', fontsize=12)
    ax1.set_title('å„ä¸»æˆåˆ†çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='1% é˜ˆå€¼')
    ax1.legend()

    # å³å›¾ï¼šç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹
    ax2.plot(components, cumulative_variance_ratio * 100, 'o-', color='steelblue', linewidth=2)
    ax2.set_xlabel('ä¸»æˆåˆ†æ•°é‡', fontsize=12)
    ax2.set_ylabel('ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹ (%)', fontsize=12)
    ax2.set_title('ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)

    # æ·»åŠ å‚è€ƒçº¿
    for threshold in [80, 85, 90, 95]:
        ax2.axhline(y=threshold, color='r', linestyle='--', alpha=0.3)
        ax2.text(max_components * 0.7, threshold + 1, f'{threshold}%', fontsize=10, color='red')

    plt.tight_layout()
    output_path = "data/evaluation/pca_variance_analysis.png"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"   âœ“ å›¾è¡¨å·²ä¿å­˜: {output_path}")

    # 8. å®é™…é™ç»´ç¤ºä¾‹
    print("\n" + "=" * 70)
    print("ğŸ¯ ä¸åŒç»´åº¦ä¸‹çš„å®é™…æ•ˆæœå¯¹æ¯”")
    print("=" * 70)

    test_dims = [5, 10, 15, 20, 30]
    for n in test_dims:
        if n <= len(cumulative_variance_ratio):
            pca_test = PCA(n_components=n, random_state=42)
            embeddings_reduced = pca_test.fit_transform(embeddings)

            # è®¡ç®—é‡æ„è¯¯å·®
            embeddings_reconstructed = pca_test.inverse_transform(embeddings_reduced)
            mse = np.mean((embeddings - embeddings_reconstructed) ** 2)
            rmse = np.sqrt(mse)

            print(f"\nç»´åº¦={n:2d} | ä¿ç•™æ–¹å·®={cumulative_variance_ratio[n-1]*100:5.2f}% | "
                  f"RMSE={rmse:.4f} | å‹ç¼©ç‡={384/n:.1f}x")

    print("\n" + "=" * 70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 70)
    print("\nå»ºè®®æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ï¼š")
    print("TEXT_PCA_COMPONENTS=<é€‰æ‹©çš„ç»´åº¦æ•°>")
    print("\næƒè¡¡å› ç´ ï¼š")
    print("  â€¢ æ›´é«˜ç»´åº¦ = æ›´å¤šä¿¡æ¯ä¿ç•™ï¼Œä½†å¢åŠ è®¡ç®—æˆæœ¬å’Œè¿‡æ‹Ÿåˆé£é™©")
    print("  â€¢ æ›´ä½ç»´åº¦ = æ›´å¿«è®¡ç®—ï¼Œä½†å¯èƒ½ä¸¢å¤±é‡è¦ä¿¡æ¯")
    print("  â€¢ æ¨èä» 10-20 ç»´å¼€å§‹ï¼Œæ ¹æ®æ¨¡å‹æ•ˆæœè°ƒæ•´")
    print()

if __name__ == "__main__":
    analyze_pca_components(max_components=50)
